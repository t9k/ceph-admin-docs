{"config":{"lang":["ja"],"separator":"[\\s\\-\uff0c\u3002]+","pipeline":["stemmer"]},"docs":[{"location":"index.html","title":"Ceph \u5b58\u50a8\u96c6\u7fa4\u7ba1\u7406\u5458\u624b\u518c","text":"<p>Ceph \u662f\u4e00\u4e2a\u9ad8\u53ef\u9760\u6027\u3001\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5f0f\u5b58\u50a8\u670d\u52a1\uff0c\u80fd\u591f\u5b58\u50a8 PB \u7ea7\u522b\u7684\u6d77\u91cf\u6570\u636e\uff0c\u5e76\u5bf9\u5916\u63d0\u4f9b\u6587\u4ef6\u7cfb\u7edf\u3001\u5bf9\u8c61\u5b58\u50a8\u3001\u5757\u5b58\u50a8\u63a5\u53e3\uff0c\u5176\u67b6\u6784\u89c1\u5b98\u65b9\u6587\u6863\u3002</p> <p>T9k AI \u5e73\u53f0\u652f\u6301\u4f7f\u7528 Ceph \u4f5c\u4e3a\u96c6\u7fa4\u5b58\u50a8\u670d\u52a1\uff0c\u5e76\u63d0\u4f9b PVC\u3001S3 \u7b49\u65b9\u5f0f\u4f7f\u7528\u3002</p> \u56fe 1\uff1a\u5982\u4f55\u5728 Kubernetes \u4e2d\u4f7f\u7528 Ceph\uff1aKubernetes \u53ef\u4ee5\u901a\u8fc7 CSI \u673a\u5236\u4ee5 PVC \u7684\u5f62\u5f0f\u8bbf\u95ee Ceph \u7684 File System \u63a5\u53e3\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7 S3 \u534f\u8bae\u8bbf\u95ee Ceph \u7684 Object Storage \u63a5\u53e3\u3002 <p>\u672c\u624b\u518c\u5728 Ceph \u5b98\u65b9\u6587\u6863\u7684\u57fa\u7840\u4e0a\u63d0\u4f9b\u66f4\u52a0\u5177\u6709\u9488\u5bf9\u6027\u7684\u6307\u5bfc\uff0c\u65b9\u4fbf\u7ba1\u7406\u5458\u5bf9 TensorStackk AI \u5e73\u53f0\u4e2d\u90e8\u7f72\u7684 Ceph \u5b58\u50a8\u96c6\u7fa4\u8fdb\u884c\u65e5\u5e38\u7ba1\u7406\u3001\u6545\u969c\u6392\u67e5\u7b49\u5de5\u4f5c\u3002</p>"},{"location":"concepts.html","title":"\u57fa\u672c\u6982\u5ff5","text":""},{"location":"concepts.html#ceph-\u67b6\u6784","title":"Ceph \u67b6\u6784","text":"<p>Ceph \u67b6\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a</p> <p>\u5728\u5e95\u5c42 RADOS (Reliable, Autonomic Distributed Object Store) \u5b58\u50a8\u670d\u52a1\u7684\u652f\u6491\u4e0b\uff0cCeph \u5bf9\u5916\u63d0\u4f9b\u4e09\u79cd\u63a5\u53e3\uff1a</p> <ol> <li>CephFS (Ceph File System): \u6587\u4ef6\u7cfb\u7edf\u63a5\u53e3\uff0c\u517c\u5bb9 POSIX\u3002</li> <li>RGW (Rados Gateway): \u5bf9\u8c61\u5b58\u50a8\u63a5\u53e3\uff0c\u517c\u5bb9 S3\u3002</li> <li>RBD (Raw Block Device): \u5757\u5b58\u50a8\u63a5\u53e3\uff0c\u63d0\u4f9b\u865a\u62df\u5757\u8bbe\u5907\u3002</li> </ol>"},{"location":"concepts.html#ceph-\u7ec4\u4ef6","title":"Ceph \u7ec4\u4ef6","text":"<p>Ceph \u4e3b\u8981\u6709\u4ee5\u4e0b\u7ec4\u4ef6\uff1a</p> <ul> <li>MON (Monitor): \u4fdd\u5b58 Ceph \u5b58\u50a8\u96c6\u7fa4\u7684\u5b8c\u6574\u4fe1\u606f\uff08\u79f0\u4e4b\u4e3a cluster map\uff09\uff0c\u4e00\u822c\u6bcf\u4e2a\u96c6\u7fa4 3~7 \u4e2a\u3002</li> <li>MGR (Manager): \u63d0\u4f9b\u76d1\u63a7\u3001\u7f16\u6392\u3001\u63d2\u4ef6\u7b49\u529f\u80fd\uff0c\u4e00\u822c\u6bcf\u4e2a\u96c6\u7fa4 2 \u4e2a\u3002</li> <li>OSD (Object Storage Device): \u7528\u4e8e\u4fdd\u5b58\u6570\u636e\uff0c\u4e00\u822c\u6bcf\u4e2a\u96c6\u7fa4 10~1000 \u4e2a\u3002<ul> <li>\u6bcf\u4e2a OSD \u5bf9\u5e94\u4e00\u4e2a HDD/SSD \u5b58\u50a8\u8bbe\u5907\u3002</li> <li>\u5ba2\u6237\u7aef\u76f4\u63a5\u5411 OSD \u53d1\u8d77 IO \u8bf7\u6c42\u3002</li> <li>OSD \u4e4b\u95f4\u4e92\u76f8\u5408\u4f5c\uff0c\u5171\u540c\u5b8c\u6210\u6570\u636e\u7684\u53ef\u9760\u5b58\u50a8\u3002</li> </ul> </li> <li>MDS (Metadata Server): \u7528\u4e8e\u5728 CephFS \u4e2d\u5b58\u50a8\u6240\u6709\u6587\u4ef6\u7684\u5143\u6570\u636e\uff0c\u4e00\u822c\u6bcf\u4e2a CephFS 2 \u4e2a\u3002</li> </ul> <p>\u5ba2\u6237\u7aef\u4e0e Ceph \u901a\u4fe1\u7684\u6b65\u9aa4\u5982\u4e0a\u56fe\u6240\u793a\uff1a</p> <ol> <li>\u5ba2\u6237\u7aef\u4ece MON \u83b7\u53d6 cluster map\uff0c\u5305\u62ec OSD \u6570\u91cf\u3001\u5730\u5740\u7b49\u4fe1\u606f\u3002</li> <li>\u5ba2\u6237\u7aef\u6839\u636e cluster map \u8ba1\u7b97\u4e00\u4e2a object \u5e94\u5f53\u5b58\u50a8\u5728\u54ea\u4e2a OSD\u3002</li> <li>\u5ba2\u6237\u7aef\u5411\u76f8\u5e94\u7684 OSD \u53d1\u8d77\u8bf7\u6c42\uff0c\u5199\u5165 object\u3002</li> <li>\u5f53\u96c6\u7fa4\u53d1\u751f\u53d8\u52a8\u65f6\uff08\u4f8b\u5982\u5b58\u50a8\u8bbe\u5907\u635f\u574f\u3001\u8282\u70b9\u589e\u52a0\uff09\uff0c\u5ba2\u6237\u7aef\u4ece MON \u83b7\u53d6\u66f4\u65b0\u7684 cluster map\uff0c\u91cd\u65b0\u8ba1\u7b97 object \u7684\u5b58\u50a8\u4f4d\u7f6e\uff0c\u5e76\u4e0e\u76f8\u5e94\u7684 OSD \u901a\u4fe1\u3002</li> <li>\u5f53\u5ba2\u6237\u7aef\u901a\u8fc7 CephFS \u63a5\u53e3\u8bbf\u95ee Ceph \u65f6\uff0c\u6587\u4ef6\u5143\u6570\u636e\u76f8\u5173\u7684\u64cd\u4f5c\uff08\u4f8b\u5982\u6253\u5f00\u6587\u4ef6\u3001\u521b\u5efa\u6587\u4ef6\u5939\uff09\u4e0e MDS \u901a\u4fe1\uff0c\u6587\u4ef6\u6570\u636e\u76f8\u5173\u7684\u64cd\u4f5c\uff08\u4f8b\u5982\u5199\u5165\u6587\u4ef6\u3001\u8bfb\u53d6\u6587\u4ef6\uff09\u4e0e\u76f8\u5e94\u7684 OSD \u901a\u4fe1\u3002</li> </ol>"},{"location":"concepts.html#rados-\u539f\u7406","title":"RADOS \u539f\u7406","text":"<p>RADOS \u5b58\u50a8\u6570\u636e\u4ee5 object \u4e3a\u5355\u4f4d\uff0c\u6bcf\u4e2a RADOS object \u7531 ID\u3001\u4e8c\u8fdb\u5236\u6570\u636e\u3001\u5143\u6570\u636e\u7ec4\u6210\uff1a</p> <p>\u5176\u4e2d\uff1a</p> <ul> <li>ID \u662f\u6bcf\u4e2a RADOS object \u7684\u5168\u5c40\u552f\u4e00\u6807\u8bc6</li> <li>\u4e8c\u8fdb\u5236\u6570\u636e\u662f RADOS object \u7684\u4e3b\u4f53\uff0c\u5927\u5c0f\u7ea6\u6570\u5341 MB</li> <li>\u5143\u6570\u636e\u662f\u4e00\u4e9b\u952e\u503c\u5bf9\uff0c\u6570\u91cf\u53ef\u4ee5\u6709\u6570\u4e07\u4e2a\uff0c\u6bcf\u4e2a\u952e\u503c\u5bf9\u7684\u5927\u5c0f\u7ea6\u6570\u5341 KB</li> </ul> <p>\u4e0b\u56fe\u5c55\u793a\u4e86\u4e00\u4e2a\u6587\u4ef6\u5b58\u5165 RADOS \u7684\u5177\u4f53\u539f\u7406\uff1a</p> <p>\u5f53\u4e00\u4e2a\u6587\u4ef6\u9700\u8981\u5b58\u5165 RADOS \u65f6\uff0c</p> <ol> <li>\u6587\u4ef6\u88ab\u5206\u4e3a\u5f88\u591a\u4e2a RADOS object</li> <li>RADOS object \u88ab\u5b58\u5165\u76f8\u5e94\u7684 Pool<ul> <li>CephFS \u5206\u4e3a medata pool \u548c data pool</li> <li>RGW \u5206\u4e3a user pool\u3001bucket pool\u3001data pool \u7b49</li> </ul> </li> <li>\u4e00\u4e2a Pool \u7531\u5f88\u591a\u4e2a PG (Placement Group) \u7ec4\u6210</li> <li>\u6bcf\u4e2a PG \u9ed8\u8ba4\u4f1a\u6709 3 \u4efd\u62f7\u8d1d\uff0c\u5b58\u50a8\u5728\u4e0d\u540c\u7684 OSD \u4e2d\uff0c\u786e\u4fdd\u5c11\u91cf OSD \u7684\u635f\u574f\u4e0d\u4f1a\u5f71\u54cd\u6570\u636e\u7684\u5b8c\u6574\u6027</li> </ol> <p>\u6839\u636e\u6570\u636e\u53ef\u9760\u6027\u8981\u6c42\u7684\u4e0d\u540c\uff0c\u6211\u4eec\u53ef\u4ee5\uff1a</p> <ul> <li>\u914d\u7f6e PG \u7684\u62f7\u8d1d\u5b58\u50a8\u5728\u4e0d\u540c\u7684 OSD \u4e2d\u3001\u4e0d\u540c\u7684\u8282\u70b9\u4e2d\u3001\u6216\u8005\u4e0d\u540c\u7684\u673a\u67b6\u4e2d\u3002</li> <li>\u914d\u7f6e PG \u62f7\u8d1d\u591a\u4efd\uff0c\u6216\u8005\u914d\u7f6e PG \u4ee5 Erasure Code \u7684\u5f62\u5f0f\u5b58\u50a8\u3002</li> </ul>"},{"location":"installation.html","title":"\u96c6\u7fa4\u5b89\u88c5","text":""},{"location":"installation.html#\u5b89\u88c5-ceph","title":"\u5b89\u88c5 Ceph","text":"<p>Ceph \u96c6\u7fa4\u7684\u5b89\u88c5\u4ece bootstrap \u4e00\u4e2a\u8282\u70b9\u5f00\u59cb\uff0c\u7136\u540e\u9010\u6b65\u52a0\u5165\u5176\u5b83\u8282\u70b9\u3001\u6dfb\u52a0\u5b58\u50a8\u8bbe\u5907\u3002</p> <p>\u9996\u5148\uff0c\u786e\u4fdd\u8282\u70b9\u4e0a\u88c5\u6709 Docker\uff0c\u5982\u679c\u6ca1\u6709\uff0c\u8bf7\u6309\u7167 Docker \u5b98\u65b9\u6587\u6863\u5b89\u88c5 Docker\u3002</p> <p>\u7136\u540e\uff0c\u5728\u7b2c\u4e00\u4e2a\u8282\u70b9\u4e0a\u5b89\u88c5 Ceph\uff1a</p> <pre><code>curl --silent --remote-name --location https://github.com/ceph/ceph/raw/quincy/src/cephadm/cephadm\nchmod +x cephadm\nsudo ./cephadm add-repo --release quincy\nsudo ./cephadm install\n\nsudo cephadm bootstrap --mon-ip &lt;ip-address-of-first-node&gt;\n\nsudo cephadm add-repo --release quincy\nsudo cephadm install ceph-common\n</code></pre> <p>\u6700\u540e\uff0c\u60a8\u53ef\u4ee5\u7ee7\u7eed\u52a0\u5165\u5176\u4ed6\u8282\u70b9\u3001\u6dfb\u52a0\u5b58\u50a8\u8bbe\u5907\u3002</p> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Cephadm Install </li> </ul>"},{"location":"installation.html#\u6dfb\u52a0\u8282\u70b9","title":"\u6dfb\u52a0\u8282\u70b9","text":"<p>\u5047\u8bbe\u65b0\u52a0\u5165\u7684\u8282\u70b9\u7684 IP \u4e3a 10.1.2.3\uff0c\u540d\u79f0\u4e3a host1\u3002</p> <p>\u9996\u5148\uff0c\u5c06 Ceph \u96c6\u7fa4\u7684 SSH \u516c\u94a5\u6dfb\u52a0\u5230\u65b0\u8282\u70b9\u7684\u4fe1\u4efb\u5217\u8868\u4e2d\u3002</p> <p>\u5982\u679c\u6709\u65b0\u8282\u70b9\u7684 root \u5bc6\u7801\uff0c\u5728\u4efb\u610f\u7ba1\u7406\u8282\u70b9\u4e0a\u8fd0\u884c\uff1a</p> <pre><code>sudo ssh-copy-id -f -i /etc/ceph/ceph.pub root@10.1.2.3\n</code></pre> <p>\u5982\u679c\u6709\u65b0\u8282\u70b9\u7684\u67d0\u4e2a\u5177\u6709 sudo \u6743\u9650\u7684\u7528\u6237\u5bc6\u7801\uff0c\u5047\u8bbe\u5176\u7528\u6237\u540d\u4e3a superuser\u3002</p> <p>\u5728\u4efb\u610f\u7ba1\u7406\u8282\u70b9\u4e0a\u8fd0\u884c\uff1a</p> <pre><code>sudo scp /etc/ceph/ceph.pub superuser@10.1.2.3:~/ceph.pub\n</code></pre> <p>\u5728\u65b0\u8282\u70b9\u4e0a\u8fd0\u884c\uff1a</p> <pre><code>sudo su\ncat ~/ceph.pub &gt;&gt; /root/.ssh/authorized_keys\n</code></pre> <p>\u7136\u540e\uff0c\u786e\u4fdd\u65b0\u8282\u70b9\u4e0a\u5b89\u88c5\u4e86 Docker\uff1a</p> <pre><code>sudo docker version\n</code></pre> <p>\u5982\u679c\u6ca1\u6709\uff0c\u8bf7\u6309\u7167 Docker \u5b98\u65b9\u6587\u6863\u5b89\u88c5 Docker\u3002</p> <p>\u6700\u540e\uff0c\u5728\u7ba1\u7406\u8282\u70b9\u4e0a\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6dfb\u52a0\u65b0\u8282\u70b9\u5230 Ceph \u96c6\u7fa4\uff1a</p> <pre><code>sudo ceph orch host add host1 10.1.2.3\nsudo ceph orch host add host1 10.1.2.3  --labels _admin\n</code></pre> <p>\u5e76\u5728\u65b0\u8282\u70b9\u4e2d\u5b89\u88c5 Ceph \u547d\u4ee4\u884c\uff1a</p> <pre><code>curl --silent --remote-name --location https://github.com/ceph/ceph/raw/quincy/src/cephadm/cephadm\nchmod +x cephadm\nsudo ./cephadm add-repo --release quincy --repo-url https://mirrors.aliyun.com/ceph # speed up using aliyun mirror\nsudo apt update\nsudo ./cephadm install ceph-common ceph\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Adding Hosts </li> </ul>"},{"location":"installation.html#\u6dfb\u52a0\u5b58\u50a8\u8bbe\u5907","title":"\u6dfb\u52a0\u5b58\u50a8\u8bbe\u5907","text":"<p>\u7528\u4e8e Ceph \u5b58\u50a8\u7684\u5b58\u50a8\u8bbe\u5907\u9700\u8981\u6ee1\u8db3\u4e0b\u5217\u8981\u6c42\uff1a</p> <ul> <li>\u6ca1\u6709\u5206\u533a</li> <li>\u6ca1\u6709 LVM \u72b6\u6001</li> <li>\u6ca1\u6709\u88ab\u6302\u8f7d</li> <li>\u6ca1\u6709\u6587\u4ef6\u7cfb\u7edf</li> <li>\u5927\u4e8e 5GB</li> </ul> <p>\u4ec0\u4e48\u662f LVM\uff1f</p> <p>Logical Volume Manager \u53ef\u4ee5\u5c06 Linux \u4e2d\u7684\u7269\u7406\u786c\u76d8\u865a\u62df\u5316\u4e3a\u903b\u8f91\u5377\uff0c\u4ee5\u4fbf\u66f4\u7075\u6d3b\u5730\u7ba1\u7406\u548c\u4f7f\u7528\u5b58\u50a8\u3002\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)</li> <li>https://wiki.archlinux.org/title/LVM</li> </ul> <p>\u5982\u679c\u5b58\u50a8\u8bbe\u5907\u66fe\u88ab\u7528\u4e8e\u5176\u4ed6 Ceph \u96c6\u7fa4\uff0c\u53ef\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u6e05\u9664\u8bbe\u5907\u72b6\u6001\uff1a</p> <pre><code>sudo ceph orch device zap &lt;hostname&gt; &lt;devicepath&gt;\n</code></pre> <p>\u4f8b\u5982\uff1a</p> <pre><code>sudo ceph orch device zap host1 /dev/sdb\n</code></pre> <p>\u5982\u679c\u5b58\u50a8\u8bbe\u5907\u66fe\u88ab\u7528\u4f5c\u5176\u4ed6\u7528\u9014\uff0c\u53ef\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5220\u9664\u8bbe\u5907\u4e2d\u7684\u5206\u533a\uff1a</p> <pre><code># \u5220\u9664\u5206\u533a\nsudo fdisk /dev/sdX\n\n# \u6e05\u9664\u6b8b\u4f59\u7684\u5206\u533a\u8868\u4fe1\u606f\nsudo sgdisk --zap-all /dev/sdX\n</code></pre> <p>\u5982\u679c\u8bbe\u5907\u4e2d\u4ecd\u7136\u6709\u6b8b\u7559\u7684 LVM \u72b6\u6001\uff0c\u53ef\u5c1d\u8bd5\uff1a</p> <ul> <li>\u91cd\u542f\u8282\u70b9</li> <li>\u5229\u7528 LVM \u76f8\u5173\u547d\u4ee4\u6e05\u9664</li> </ul> <p>\u5728\u4efb\u610f\u7ba1\u7406\u8282\u70b9\u4e0a\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5373\u53ef\u81ea\u52a8\u5316\u5730\u53d1\u73b0\u6240\u6709\u8282\u70b9\u4e0a\u53ef\u7528\u7684\u5b58\u50a8\u8bbe\u5907\uff0c\u5e76\u4e3a\u5176\u521b\u5efa OSD Daemon\u3001\u6dfb\u52a0\u5230 Ceph \u96c6\u7fa4\u4e2d\uff1a</p> <pre><code>sudo ceph orch apply osd --all-available-devices\n</code></pre> <p>\u6216\u8005\uff0c\u505c\u6b62\u81ea\u52a8\u53d1\u73b0\uff1a</p> <pre><code>sudo ceph orch apply osd --all-available-devices --unmanaged=true\n</code></pre> <p>\u7136\u540e\u624b\u52a8\u4e3a\u67d0\u4e2a\u5b58\u50a8\u8bbe\u5907\u521b\u5efa OSD\uff1a</p> <pre><code>sudo ceph orch daemon add osd &lt;hostname&gt;:&lt;device-path&gt;\n</code></pre> <p>\u4f8b\u5982\uff1a</p> <pre><code>sudo ceph orch daemon add osd host1:/dev/sdb\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Deploy OSDs </li> </ul>"},{"location":"installation.html#mpath-\u8bbe\u5907","title":"mpath \u8bbe\u5907","text":"<p>\u5982\u679c\u5b58\u50a8\u8bbe\u5907\u4e0d\u662f\u5e38\u89c1\u7684 hdd\u3001ssd \u786c\u76d8\uff0c\u800c\u662f mpath \u8bbe\u5907\uff0c\u60a8\u9700\u8981\u624b\u52a8\u521b\u5efa osd\u3002</p> <p>\u4ec0\u4e48\u662f mpath \u8bbe\u5907\uff1f</p> <p>Multipath \u662f\u4e00\u79cd\u4e3a\u4e3b\u673a\u548c\u5b58\u50a8\u786c\u4ef6\u4e4b\u95f4\u7684\u7f51\u7edc\u901a\u4fe1\u63d0\u4f9b\u5bb9\u9519\u7684\u6280\u672f\u3002\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>https://en.wikipedia.org/wiki/Linux_DM_Multipath</li> <li>https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/dm_multipath/mpath_devices</li> <li>https://ubuntu.com/server/docs/device-mapper-multipathing-introduction</li> </ul> <p>\u5047\u8bbe mpath \u8bbe\u5907\u4f4d\u4e8e /dev/mapper/mpatha\u3002\u9996\u5148\uff0c\u521b\u5efa lvm \u76f8\u5173\u7684 pv\u3001vg\u3001lv \u7b49\uff1a</p> <pre><code>sudo pvcreate --metadatasize 250k -y -ff /dev/mapper/mpatha\nsudo vgcreate vgmpatha /dev/mapper/mpatha\nsudo lvcreate -n lv0 -l 100%FREE vgmpatha\n</code></pre> <p>\u7136\u540e\uff0c\u4f7f\u7528 ceph \u547d\u4ee4\u884c\u5de5\u5177\u51c6\u5907 lv\uff1a</p> <pre><code>sudo ceph-volume lvm prepare --data /dev/vgmpatha/lv0\n</code></pre> <p>\u6700\u540e\uff0c\u6fc0\u6d3b osd\uff1a</p> <pre><code>sudo ceph-volume lvm activate --all\n</code></pre> <p>\u6fc0\u6d3b\u65f6\u53ef\u80fd\u9700\u8981\u6307\u5b9a\u5177\u4f53\u7684 osd\uff1a</p> <pre><code>sudo ceph-volume lvm activate &lt;osd-id&gt; &lt;osd-fsid&gt;\n</code></pre> <p>\u4f8b\u5982\uff1a</p> <pre><code>sudo ceph-volume lvm activate 3 249e9103-5fec-4acc-b545-82c644f8756f\n</code></pre> <p>\u5176\u4e2d\uff0c<code>&lt;osd-id&gt;</code> \u548c <code>&lt;osd-fsid&gt;</code> \u5728\u4f7f\u7528 ceph \u547d\u4ee4\u884c\u5de5\u5177\u51c6\u5907 lv \u6b65\u9aa4\u7684\u8f93\u51fa\u4e2d\u53ef\u4ee5\u627e\u5230\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\uff1a</p> <pre><code>sudo ceph-volume lvm list \n</code></pre> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u53ef\u4ee5\u67e5\u770b lvm \u76f8\u5173\u7684 pv\u3001vg\u3001lv\uff1a</p> <pre><code>sudo pvs\nsudo pvdisplay\nsudo vgs\nsudo vgdisplay\nsudo lvs\nsudo lvdisplay\n</code></pre> <p>\u5f53 mpath \u8bbe\u5907\u6240\u5728\u7684\u8282\u70b9\u91cd\u542f\u65f6\uff0clv \u53ef\u80fd\u53d8\u4e3a unavailable \u72b6\u6001\uff0c\u5bfc\u81f4 osd \u5f02\u5e38\uff0c\u9700\u8981\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u91cd\u65b0\u6fc0\u6d3b lv\uff1a</p> <pre><code>sudo lvchange -ay /dev/vgmpatha/lv0\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>https://forum.proxmox.com/threads/ceph-with-multipath.70813/post-317961</li> </ul>"},{"location":"installation.html#\u914d\u7f6e-dashboard","title":"\u914d\u7f6e dashboard","text":"<p>Ceph \u63d0\u4f9b\u53ef\u89c6\u5316 dashboard \u6765\u7ba1\u7406\u96c6\u7fa4\u3002</p> <p>\u542f\u7528 dashboard\uff1a</p> <pre><code>sudo ceph mgr module enable dashboard\n</code></pre> <p>\u8bbe\u7f6e http \u8bbf\u95ee\uff1a \\</p> <pre><code>sudo ceph config set mgr mgr/dashboard/ssl false\n</code></pre> <p>\u91cd\u542f dashboard\uff1a \\</p> <pre><code>sudo ceph mgr module disable dashboard\nsudo ceph mgr module enable dashboard\n</code></pre> <p>\u8bbe\u7f6e dashboard \u5bc6\u7801\uff1a</p> <pre><code>sudo ceph dashboard ac-user-set-password admin -i &lt;file-containing-password&gt;\n</code></pre> <p>\u53c2\u8003\u67e5\u770b daemon \u72b6\u6001\uff0c\u627e\u5230 mgr daemon \u6240\u5728\u7684\u8282\u70b9\uff0cdashboard \u7684\u5730\u5740\u5373\u4e3a <code>http://&lt;mgr-host-ip&gt;:8080</code>\u3002</p> <p>\u4e3a\u4e86\u914d\u7f6e grafana \u4f7f\u7528 http\uff0c\u901a\u8fc7 SSH \u767b\u5f55 grafana \u6240\u5728\u7684\u8282\u70b9\uff0c\u4fee\u6539 grafana \u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u5c06 https \u6539\u4e3a http\uff0c\u7136\u540e\u91cd\u542f grafana \u5bb9\u5668\uff1a</p> <pre><code>sudo vim /var/lib/ceph/&lt;cluster-id&gt;/grafana.&lt;hostname&gt;/etc/grafana/grafana.ini\ndocker ps | grep grafana\ndocker restart &lt;grafana-container&gt;\n</code></pre> <p>\u914d\u7f6e grafana\u3001alertmanager\u3001prometheus \u7b49\u76d1\u63a7\u7ec4\u4ef6\uff0c\u4ee5\u4fbf\u5728 dashboard \u4e2d\u67e5\u770b\u53ef\u89c6\u5316\u7edf\u8ba1\u56fe\u8868\uff1a</p> <pre><code>sudo ceph dashboard set-grafana-frontend-api-url http://&lt;host-ip&gt;:3000\nsudo ceph dashboard set-grafana-api-ssl-verify False\nsudo ceph dashboard set-grafana-api-url http://&lt;host-ip&gt;:3000\nsudo ceph dashboard set-alertmanager-api-host http://&lt;host-ip&gt;:9093\nsudo ceph dashboard set-alertmanager-api-ssl-verify false\nsudo ceph dashboard set-prometheus-api-host http://&lt;host-ip&gt;:9095\nsudo ceph dashboard set-prometheus-api-ssl-verify false\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Dashboard</li> </ul>"},{"location":"installation.html#\u914d\u7f6e-crushmap","title":"\u914d\u7f6e crushmap","text":"<p>Ceph \u7684\u9ed8\u8ba4\u5b58\u50a8\u7b56\u7565\u662f\u5728\u4e0d\u540c\u7684\u8282\u70b9\u4e4b\u95f4\u5c06\u6570\u636e\u590d\u5236 3 \u4efd\uff0c\u4ee5\u63d0\u4f9b\u9ad8\u53ef\u7528\u6027\u3002\u5982\u679c\u6211\u4eec\u53ea\u6709\u5c11\u4e8e 3 \u4e2a\u8282\u70b9\uff0c\u6839\u636e\u8fd9\u4e2a\u535a\u5ba2\uff0c\u9700\u8981\u914d\u7f6e Ceph \u5728 OSD \u4e4b\u95f4\u590d\u5236\u6570\u636e\u3002</p> <p>\u6ce8\u610f</p> <p>\u5728\u591a\u4e8e 3 \u4e2a\u8282\u70b9\u7684\u751f\u4ea7\u96c6\u7fa4\u4e2d\uff0c\u63a8\u8350\u914d\u7f6e\u4ecd\u7136\u662f\u5728\u8282\u70b9\u4e4b\u95f4\u590d\u5236\u6570\u636e\uff0c\u4ee5\u907f\u514d\u4e00\u4e2a\u8282\u70b9\u6545\u969c\u5bfc\u81f4\u67d0\u4e9b\u6570\u636e\u7684\u6c38\u4e45\u4e22\u5931\u3002</p> <p>\u67e5\u770b\u5f53\u524d\u7684 crushmap\uff1a</p> <pre><code>sudo ceph osd crush rule dump\n</code></pre> <p>\u5bfc\u51fa\u5f53\u524d\u7684 crushmap\uff1a</p> <pre><code>sudo ceph osd getcrushmap -o comp_crush_map.cm\n</code></pre> <p>\u5c06\u5bfc\u51fa\u7684 curshmap \u89e3\u6790\u4e3a\u4eba\u7c7b\u53ef\u8bfb\u7684\u6587\u4ef6\uff1a </p> <pre><code>crushtool -d comp_crush_map.cm -o crush_map.cm\n</code></pre> <p>\u7f16\u8f91 crushmap\uff0c\u5c06\u5176\u4e2d\u7684 host \u6539\u4e3a osd\uff1a </p> <pre><code>vim crush_map.cm\n</code></pre> <p>\u4f8b\u5982\uff0c\u5bfb\u627e crushmap \u4e2d\u7684\u4e0b\u5217\u5185\u5bb9\uff1a</p> <pre><code># rules\nrule replicated_rule {\n    id 0\n    type replicated\n    step take default\n    step chooseleaf firstn 0 type host\n    step emit\n}\n</code></pre> <p>\u5c06\u5176\u4fee\u6539\u4e3a\uff1a</p> <pre><code># rules\nrule replicated_rule {\n    id 0\n    type replicated\n    step take default\n    step chooseleaf firstn 0 type osd\n    step emit\n}\n</code></pre> <p>\u7f16\u8bd1\u65b0\u7684 crushmap\uff1a</p> <pre><code>crushtool -c crush_map.cm -o new_crush_map.cm\n</code></pre> <p>\u5e94\u7528\u65b0\u7684 curshmap\uff1a \\</p> <pre><code>sudo ceph osd setcrushmap -i new_crush_map.cm\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - CRUSH Maps</li> <li>Ceph - Manaully editing a CRUSH map</li> </ul>"},{"location":"installation.html#\u914d\u7f6e\u65f6\u949f\u540c\u6b65","title":"\u914d\u7f6e\u65f6\u949f\u540c\u6b65","text":"<p>\u6211\u4eec\u63a8\u8350\u4f7f\u7528 chrony \u6765\u540c\u6b65\u6240\u6709\u8282\u70b9\u7684\u65f6\u949f\uff0c\u63a8\u8350\u91c7\u7528\u4e00\u4e2a\u4e3b\u8282\u70b9\u5411\u5916\u90e8\u65f6\u949f\u670d\u52a1\u5668\u540c\u6b65\u65f6\u949f\u3001\u5176\u4ed6\u8282\u70b9\u5411\u8be5\u4e3b\u8282\u70b9\u540c\u6b65\u65f6\u949f\u7684\u65b9\u6848\u3002</p> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5 chrony\uff1a</p> <pre><code>sudo apt update\nsudo apt install chrony\n</code></pre> <p>chrony \u914d\u7f6e\u6587\u4ef6\u4f4d\u4e8e /etc/chrony/chrony.conf</p> <p>\u4e3b\u8282\u70b9\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code># Welcome to the chrony configuration file. See chrony.conf(5) for more\n# information about usuable directives.\n\n# This will use (up to):\n# - 4 sources from ntp.ubuntu.com which some are ipv6 enabled\n# - 2 sources from 2.ubuntu.pool.ntp.org which is ipv6 enabled as well\n# - 1 source from [01].ubuntu.pool.ntp.org each (ipv4 only atm)\n# This means by default, up to 6 dual-stack and up to 2 additional IPv4-only\n# sources will be used.\n# At the same time it retains some protection against one of the entries being\n# down (compare to just using one of the lines). See (LP: #1754358) for the\n# discussion.\n#\n# About using servers from the NTP Pool Project in general see (LP: #104525).\n# Approved by Ubuntu Technical Board on 2011-02-08.\n# See http://www.pool.ntp.org/join.html for more information.\npool ntp.ubuntu.com        iburst maxsources 4\npool 0.ubuntu.pool.ntp.org iburst maxsources 1\npool 1.ubuntu.pool.ntp.org iburst maxsources 1\npool 2.ubuntu.pool.ntp.org iburst maxsources 2\n\nallow 100.64.4.0/8\n\n# This directive specify the location of the file containing ID/key pairs for\n# NTP authentication.\nkeyfile /etc/chrony/chrony.keys\n\n# This directive specify the file into which chronyd will store the rate\n# information.\ndriftfile /var/lib/chrony/chrony.drift\n\n# Uncomment the following line to turn logging on.\n#log tracking measurements statistics\n\n# Log files location.\nlogdir /var/log/chrony\n\n# Stop bad estimates upsetting machine clock.\nmaxupdateskew 100.0\n\n# This directive enables kernel synchronisation (every 11 minutes) of the\n# real-time clock. Note that it can't be used along with the 'rtcfile' directive.\nrtcsync\n\n# Step the system clock instead of slewing it if the adjustment is larger than\n# one second, but only in the first three clock updates.\nmakestep 1 3\n</code></pre> <p>\u6ce8\u610f\uff0c\u5176\u4e2d <code>allow 100.64.4.0/8</code> \u9700\u586b\u5199\u5176\u4ed6\u8282\u70b9\u7684 ip \u7f51\u6bb5\u3002</p> <p>\u5176\u4ed6\u8282\u70b9\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code># Welcome to the chrony configuration file. See chrony.conf(5) for more\n# information about usuable directives.\n\n# This will use (up to):\n# - 4 sources from ntp.ubuntu.com which some are ipv6 enabled\n# - 2 sources from 2.ubuntu.pool.ntp.org which is ipv6 enabled as well\n# - 1 source from [01].ubuntu.pool.ntp.org each (ipv4 only atm)\n# This means by default, up to 6 dual-stack and up to 2 additional IPv4-only\n# sources will be used.\n# At the same time it retains some protection against one of the entries being\n# down (compare to just using one of the lines). See (LP: #1754358) for the\n# discussion.\n#\n# About using servers from the NTP Pool Project in general see (LP: #104525).\n# Approved by Ubuntu Technical Board on 2011-02-08.\n# See http://www.pool.ntp.org/join.html for more information.\nserver 100.64.4.104\n\n# This directive specify the location of the file containing ID/key pairs for\n# NTP authentication.\nkeyfile /etc/chrony/chrony.keys\n\n# This directive specify the file into which chronyd will store the rate\n# information.\ndriftfile /var/lib/chrony/chrony.drift\n\n# Uncomment the following line to turn logging on.\n#log tracking measurements statistics\n\n# Log files location.\nlogdir /var/log/chrony\n\n# Stop bad estimates upsetting machine clock.\nmaxupdateskew 100.0\n\n# This directive enables kernel synchronisation (every 11 minutes) of the\n# real-time clock. Note that it can't be used along with the 'rtcfile' directive.\nrtcsync\n\n# Step the system clock instead of slewing it if the adjustment is larger than\n# one second, but only in the first three clock updates.\nmakestep 1 3\n</code></pre> <p>\u6ce8\u610f\uff0c\u5176\u4e2d <code>server 100.64.4.104</code> \u9700\u586b\u5199\u4e3b\u8282\u70b9\u7684 ip\u3002</p> <p>\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u540e\uff0c\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u91cd\u542f chrony \u4f7f\u914d\u7f6e\u751f\u6548\uff1a</p> <pre><code>sudo systemctl restart chronyd\n</code></pre> <p>\u5728\u4e3b\u8282\u70b9\u4e0a\u67e5\u770b chrony \u8fd0\u884c\u72b6\u6001\uff1a</p> <pre><code>sudo chronyc activity\nsudo chronyc tracking\nsudo chronyc sources -v\nsudo chronyc clients\n</code></pre> <p>\u67e5\u770b Ceph \u65f6\u949f\u540c\u6b65\u72b6\u6001\uff1a</p> <pre><code>sudo ceph time-sync-status\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>https://ubuntu.com/blog/ubuntu-bionic-using-chrony-to-configure-ntp</li> </ul>"},{"location":"installation.html#\u914d\u7f6e\u8b66\u544a\u901a\u77e5","title":"\u914d\u7f6e\u8b66\u544a\u901a\u77e5","text":"<p>Ceph \u81ea\u5e26\u4e00\u6574\u5957\u76d1\u63a7\u7cfb\u7edf\uff0c\u5305\u62ec prometheus/grafana/alert mamanger\u3002\u4e3a\u4e86\u80fd\u591f\u53ca\u65f6\u6536\u5230 alert manager \u53d1\u51fa\u7684\u8b66\u544a\uff0c\u60a8\u53ef\u4ee5\u914d\u7f6e alert manager \u901a\u8fc7\u90ae\u7bb1\u548c\u4f01\u4e1a\u5fae\u4fe1\u6765\u53d1\u9001\u8b66\u544a\u4fe1\u606f\u3002</p> <p>\u901a\u8fc7 SSH \u767b\u5f55 alert manager \u6240\u5728\u7684\u8282\u70b9\uff0c\u5c06\u4f4d\u4e8e <code>/var/lib/ceph/&amp;lt;cluster-id&gt;/alertmanager.&amp;lt;hostname&gt;/etc/alertmanager</code> \u7684\u914d\u7f6e\u6587\u4ef6 alertmanager.yml \u4fee\u6539\u4e3a\u5982\u4e0b\u5185\u5bb9\uff08\u6ce8\u610f\u66ff\u6362\u5176\u4e2d\u90ae\u7bb1\u548c\u4f01\u4e1a\u5fae\u4fe1\u76f8\u5173\u7684\u5177\u4f53\u914d\u7f6e\uff09\uff1a</p> <pre><code># This file is generated by cephadm.\n# See https://prometheus.io/docs/alerting/configuration/ for documentation.\n\nglobal:\n  resolve_timeout: 5m\n  http_config:\n    tls_config:\n      insecure_skip_verify: true\n\nroute:\n  receiver: 'ceph-dashboard'\n  routes:\n    - group_by: ['alertname']\n      group_wait: 10s\n      group_interval: 10s\n      repeat_interval: 1h\n      receiver: 'ceph-dashboard'\n      continue: true\n    - group_by: ['alertname']\n      group_wait: 30s\n      group_interval: 5m\n      repeat_interval: 6h\n      receiver: 't9k-monitoring/email/t9k-sre'\n      continue: true\n    - group_by: ['alertname']\n      group_wait: 10s\n      group_interval: 5m\n      repeat_interval: 12h\n      receiver: 't9k-monitoring/wechat/alerts'\n      continue: true\n\nreceivers:\n- name: 'ceph-dashboard'\n  webhook_configs:\n  - url: 'http://&lt;hostname&gt;:8080/api/prometheus_receiver'\n- name: t9k-monitoring/email/t9k-sre\n  email_configs:\n  - to: &lt;to&gt;\n    from: &lt;from&gt;\n    smarthost: &lt;smarthost&gt;\n    auth_username: &lt;auth_username&gt;\n    auth_password: &lt;auth_password&gt;\n    headers:\n      Subject: '{{ template \"email.subject\" . }}'\n- name: t9k-monitoring/wechat/alerts\n  wechat_configs:\n  - api_secret: &lt;api_secret&gt;\n    corp_id: &lt;corp_id&gt;\n    agent_id: &lt;agent_id&gt;\n    to_user: '@all'\n    message: '{{ template \"wechat.message\" . }}'\ntemplates:\n- /etc/alertmanager/custom.tmpl\n</code></pre> <p>\u5728\u540c\u4e00\u6587\u4ef6\u5939\u4e0b\u521b\u5efa custom.tmpl\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a</p> <pre><code>{{ define \"email.subject\" }}[ceph]{{ template \"__subject\" . }}{{ end }}\n\n{{ define \"__custom_text_alert_list\" }}{{ range . }}\n--\n{{with .Annotations.description}}Description:  {{.}} {{else -}} {{end}}\n\nLabels:\n{{- range .Labels.SortedPairs }}\n- {{ .Name }}:\n   {{ .Value }}\n{{- end }}\n{{- end }}{{- end -}}\n\n{{ define \"wechat.message\" }}[ceph]{{ template \"__subject\" . }}\n\nSummary: {{ .CommonAnnotations.summary }}\n{{ if gt (len .Alerts.Firing) 0 }}\nFiring Alerts:\n{{ template \"__custom_text_alert_list\" .Alerts.Firing }}\n{{- end }}\n{{- if gt (len .Alerts.Resolved) 0}}\nResolved Alerts:\n{{ template \"__custom_text_alert_list\" .Alerts.Resolved }}\n{{- end }}{{- end -}}\n</code></pre> <p>\u7136\u540e\u91cd\u542f alert manager \u5bb9\u5668\uff1a</p> <pre><code>docker ps | grep alertmanager\ndocker restart &lt;alert-manager-container&gt;\n</code></pre>"},{"location":"installation.html#\u521b\u5efa-cephfs","title":"\u521b\u5efa cephfs","text":"<p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efa cephfs\uff1a</p> <pre><code>sudo ceph fs volume create &lt;fs_name&gt; --placement=&lt;placement&gt;\n</code></pre> <p>\u5176\u4e2d <code>&lt;placement&gt;</code>\u662f\u6307 cephfs \u7684 mds daemon \u90e8\u7f72\u5728\u54ea\u4e9b\u8282\u70b9\u4e0a\u3002\u4e3a\u4e86\u9ad8\u53ef\u7528\uff0c\u4e00\u4e2a cephfs \u4e00\u822c\u6709\u4e24\u4e2a mds daemon\uff1a\u4e00\u4e2a\u6b63\u5e38\u8fd0\u884c\uff0c\u4e00\u4e2a\u968f\u65f6\u51c6\u5907\u63a5\u66ff\u3002</p> <p>\u4f8b\u5982\uff1a</p> <pre><code>sudo ceph fs volume create k8s --placement=\"host1 host2\"\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Deploy CephFS</li> </ul>"},{"location":"installation.html#\u914d\u7f6e-erasure-code","title":"\u914d\u7f6e erasure code","text":"<p>erasure code \u662f\u4e00\u79cd\u7ea0\u9519\u7801\uff0c\u5728\u539f\u59cb\u6570\u636e\u7684\u57fa\u7840\u4e0a\u5197\u4f59\u5b58\u50a8\u4e00\u4e9b\u7ecf\u8fc7\u7f16\u7801\u7684\u6570\u636e\u5757\uff0c\u4ee5\u4fbf\u5728\u4e00\u4e9b\u6570\u636e\u5757\u635f\u574f\u65f6\u4ecd\u7136\u80fd\u591f\u8ba1\u7b97\u51fa\u539f\u59cb\u6570\u636e\u3002</p> <p>erasure code \u6700\u91cd\u8981\u7684\u53c2\u6570\u662f k \u548c m\uff0c\u5176\u4e2d</p> <ul> <li>k \u8868\u793a\u539f\u59cb\u6570\u636e\u5c06\u88ab\u5206\u4e3a k \u4e2a\u6570\u636e\u5757\uff08data chunks\uff09</li> <li>m \u8868\u793a\u5728\u539f\u59cb\u6570\u636e\u7684\u57fa\u7840\u4e0a\u8ba1\u7b97\u51fa m \u4e2a\u7f16\u7801\u5757\uff08encoding chunks\uff09</li> </ul> <p>\u4e0a\u8ff0 k + m \u4e2a\u5757\u5c06\u4f1a\u88ab\u5206\u522b\u5b58\u50a8\u5728\u4e0d\u540c\u7684\u5730\u65b9\uff0c\u6839\u636e\u914d\u7f6e\u7684\u4e0d\u540c\uff0c\u53ef\u4ee5\u662f\u4e0d\u540c\u7684 host\u3001\u4e0d\u540c\u7684 osd \u7b49\u3002</p> <p>erasure code \u7684\u4f18\u52bf\u5728\u4e8e</p> <ul> <li>\u6700\u591a\u80fd\u627f\u53d7 k + m \u4e2a\u5757\u4e2d\u4efb\u610f m \u4e2a\u5757\u7684\u635f\u574f</li> <li>\u5b58\u50a8 1 \u4e2a\u5355\u4f4d\u7684\u539f\u59cb\u6570\u636e\uff0c\u9700\u8981 (k + m) / k \u4e2a\u5355\u4f4d\u7684\u7a7a\u95f4</li> </ul> <p>\u4f5c\u4e3a\u53c2\u8003\uff0cRed Hat \u652f\u6301\u4ee5\u4e0b k/m \u503c\uff1a</p> <ul> <li>k=8 m=3</li> <li>k=8 m=4</li> <li>k=4 m=2</li> </ul> <p>\u9996\u5148\u521b\u5efa\u4e00\u4e2a erasure code profile\uff08\u4ee5 k=4 m=2 \u4e3a\u4f8b\uff09\uff1a</p> <pre><code>sudo ceph osd erasure-code-profile set ecprofile-k4-m2 k=4 m=2 crush-failure-domain=osd\nsudo ceph osd erasure-code-profile ls\nsudo ceph osd erasure-code-profile get ecprofile-k4-m2\n</code></pre> <p>\u7136\u540e\u521b\u5efa\u4e00\u4e2a erasure coded pool\uff1a</p> <pre><code>sudo ceph osd pool create ecpool-k4-m2 erasure ecprofile-k4-m2\nsudo ceph osd pool set ecpool-k4-m2 allow_ec_overwrites true\nsudo ceph osd pool application enable ecpool-k4-m2 cephfs\n</code></pre> <p>\u6700\u540e\u5c06 erasure coded pool \u4f5c\u4e3a\u7b2c\u4e8c\u4e2a data pool \u52a0\u5165\u5230 CephFS \u4e2d\uff1a</p> <pre><code>sudo ceph fs add_data_pool &lt;fs-name&gt; ecpool-k4-m2\nsudo ceph fs ls\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Erasure code</li> <li>Ceph - Using erasure coded pools with CephFS</li> <li>Ceph - Adding a data pool to the file system</li> </ul>"},{"location":"k8s.html","title":"K8s \u96c6\u6210","text":"<p>\u901a\u8fc7 Ceph CSI\uff0cK8s \u53ef\u4ee5\u5229\u7528 Ceph \u4f5c\u4e3a PVC \u7684\u5b58\u50a8\u63d0\u4f9b\u8005\uff08Provisioner\uff09\u3002</p>"},{"location":"k8s.html#\u914d\u7f6e-ceph-csi","title":"\u914d\u7f6e Ceph CSI","text":"<p>\u9996\u5148\uff0c\u5728 Ceph \u96c6\u7fa4\u4e2d\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a demo-fs \u7684 Ceph File System \u548c\u4e00\u4e2a\u540d\u4e3a demo-user \u7684 Ceph \u7528\u6237\uff0c\u5728\u7ba1\u7406\u8282\u70b9\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5373\u53ef\uff1a</p> <pre><code>$ sudo ceph fs volume create demo-fs\n$ sudo ceph auth get-or-create client.demo-user mon 'allow r' \\\n  osd 'allow rw tag cephfs *=*' mgr 'allow rw' mds 'allow rw'\n[client.demo-user]\n    key = AQAhuZBjSuS9AxBD77AvVjr+vAg9zhjRK7NR+g==\n</code></pre> <p>\u6ce8\u610f\u4e0a\u8ff0\u751f\u6210\u7684 key \u503c\u540e\u9762\u4f1a\u7528\u5230\u3002</p> <p>\u7136\u540e\uff0c\u67e5\u770b Ceph \u96c6\u7fa4\u7684\u4e00\u4e9b\u76f8\u5173\u4fe1\u606f\uff0c\u5728\u7ba1\u7406\u8282\u70b9\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5373\u53ef\uff1a</p> <pre><code>$ sudo ceph mon dump\nepoch 2\nfsid 47f20cbc-914f-24ed-93dc-9f2800951ba2\nlast_changed 2023-01-12T06:01:55.659954+0000\ncreated 2023-01-11T01:30:12.337519+0000\nmin_mon_release 17 (quincy)\nelection_strategy: 1\n0: [v2:10.0.0.1:3300/0,v1:10.0.0.1:6789/0] mon.ds01\n1: [v2:10.0.0.2:3300/0,v1:10.0.0.2:6789/0] mon.e01\ndumped monmap epoch 2\n</code></pre> <p>\u6ce8\u610f\u4e0a\u8ff0\u8f93\u51fa\u4e2d\u7684 fsid (47f20cbc-914f-24ed-93dc-9f2800951ba2) \u548c\u8282\u70b9\u5217\u8868 (10.0.0.1:6789, 10.0.0.2:6789)\uff0c\u540e\u9762\u4f1a\u7528\u5230\u3002</p> <p>\u6700\u540e\uff0c\u5728 K8s \u96c6\u7fa4\u4e2d\u521b\u5efa\u76f8\u5173\u8d44\u6e90\uff0c\u5305\u62ec ConfigMap\u3001Secret\u3001Deployment\u3001DaemonSet \u7b49\u7b49\u3002</p> <pre><code>git clone https://github.com/ceph/ceph-csi.git\ncd ceph-csi/examples\n</code></pre> <p>\u7f16\u8f91 csi-config-map-sample.yaml\uff0c\u586b\u5165\u4e0a\u9762\u7684 fsid \u548c\u8282\u70b9\u5217\u8868\uff0c\u9664\u6b64\u4e4b\u5916\u7684\u5176\u4ed6\u9879\u53ef\u4ee5\u5220\u9664\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>apiVersion: v1\nkind: ConfigMap\ndata:\n config.json: |-\n   [\n     {\n       \"clusterID\": \"47f20cbc-914f-24ed-93dc-9f2800951ba2\",\n       \"monitors\": [\n         \"10.0.0.1:6789\",\n         \"10.0.0.2:6789\"\n       ]\n     }\n   ]\nmetadata:\n name: ceph-csi-config\n</code></pre> <p>\u7f16\u8f91 cephfs/secret.yaml\uff0c\u586b\u5165\u4e0a\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u548c key \u503c\uff0c\u9664\u6b64\u4e4b\u5916\u7684\u5176\u4ed6\u9879\u53ef\u4ee5\u5220\u9664\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n name: csi-cephfs-secret\n namespace: default\nstringData:\n # Required for dynamically provisioned volumes\n adminID: demo-user\n adminKey: AQB7JL5jugJwFxAA+szhrjIi48JhJbZsI3feRg==\n</code></pre> <p>\u7f16\u8f91 cephfs/storageclass.yaml\uff0c\u586b\u5165\u4e0a\u9762\u7684 fsid \u548c Ceph File System \u540d\u79f0\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n name: sc-cephfs\nprovisioner: cephfs.csi.ceph.com\nparameters:\n clusterID: 47f20cbc-914f-24ed-93dc-9f2800951ba2\n fsName: demo-fs\n csi.storage.k8s.io/provisioner-secret-name: csi-cephfs-secret\n csi.storage.k8s.io/provisioner-secret-namespace: default\n csi.storage.k8s.io/controller-expand-secret-name: csi-cephfs-secret\n csi.storage.k8s.io/controller-expand-secret-namespace: default\n csi.storage.k8s.io/node-stage-secret-name: csi-cephfs-secret\n csi.storage.k8s.io/node-stage-secret-namespace: default\nreclaimPolicy: Delete\nallowVolumeExpansion: true\nmountOptions:\n - debug\n</code></pre> <p>\u6309\u7167 README.md \u4e2d\u7684\u6307\u5f15\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728 K8s \u96c6\u7fa4\u4e2d\u4f9d\u6b21\u521b\u5efa\u6240\u9700\u8d44\u6e90\uff1a</p> <pre><code>kubectl apply -f ./ceph-conf.yaml\nkubectl apply -f ./csi-config-map-sample.yaml\nkubectl apply -f ./cephfs/secret.yaml\nkubectl apply -f ./cephfs/storageclass.yaml\ncd ../../deploy/cephfs/kubernetes\nkubectl create -f ./csi-provisioner-provisioner.yaml\nkubectl create -f ./csi-nodeplugin-rbac.yaml\nkubectl create -f ./csi-cephfsplugin-provisioner.yaml\nkubectl create -f ./csi-cephfsplugin.yaml\nkubectl create -f ./csidriver.yaml\n</code></pre> <p>\u6ce8\u610f\uff0c\u4e0a\u8ff0\u8d44\u6e90\u5747\u9ed8\u8ba4\u5728 default namespace \u4e2d\u521b\u5efa\u3002</p> <p>\u7b49\u5f85\u6240\u521b\u5efa\u7684 Pod \u6210\u529f\u8fd0\u884c\u540e\uff0c\u5373\u53ef\u521b\u5efa\u57fa\u4e8e Ceph \u7684 PVC\u3002</p> <pre><code>kubectl get pod -w\n</code></pre>"},{"location":"k8s.html#\u901a\u8fc7-helm-chart-\u914d\u7f6e-ceph-csi","title":"\u901a\u8fc7 Helm Chart \u914d\u7f6e Ceph CSI","text":"<p>\u7406\u8bba\u4e0a\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u540c\u4e00\u4e2a K8s \u96c6\u7fa4\u4e2d\u90e8\u7f72\u4e24\u5957 Ceph CSI\uff0c\u63d0\u4f9b\u4e24\u4e2a Storage Class \u4ee5\u4f9b\u7528\u6237\u4f7f\u7528\u3002</p> <p>\u5728\u90e8\u7f72\u8fc7\u7a0b\u4e2d\uff1a</p> <ul> <li>\u5bf9\u4e8e Ceph \u96c6\u7fa4\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e24\u4e2a\u4e0d\u540c\u7684 Ceph Filesystem \u53ca\u5bf9\u5e94\u7684\u7528\u6237</li> <li>\u5bf9\u4e8e K8s \u96c6\u7fa4\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e24\u4e2a\u4e0d\u540c\u7684\u547d\u540d\u7a7a\u95f4\uff0c\u5e76\u5bf9 Storage Class\u3001ClusterRole\u3001ClusterRoleBinding \u7b49\u96c6\u7fa4\u7ea7\u522b\u8d44\u6e90\u4f7f\u7528\u4e0d\u540c\u7684\u540d\u79f0</li> </ul> <p>\u4e3a\u4e86\u65b9\u4fbf\u90e8\u7f72\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86 Helm Chart\uff08\u70b9\u51fb\u6b64\u5904\u4e0b\u8f7d\uff09\uff0c\u60a8\u53ea\u9700\u8981\u5728 values.yaml \u4e2d\u586b\u5199\u53c2\u6570\u5373\u53ef\u3002</p> <p>values.yaml \u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>ceph:\n  storageClassName: sc-cephfs\n  driverName: cephfs.csi.ceph.com\n  clusterID: 47f20cbc-914f-24ed-93dc-9f2800951ba2\n  fsName: demo-fs\n  adminID: demo-user\n  adminKey: AQAhuZBjSuS9AxBD77AvVjr+vAg9zhjRK7NR+g==\n  metricsPort: 8681\n  monitors:\n    - \"10.0.0.1:6789\"\n    - \"10.0.0.2:6789\"\n  # erasureCode: true\n  # pool: ecpool-k4-m2\n  images:\n    cephCSI: quay.io/cephcsi/cephcsi:v3.8.0\n    csiProvisioner: registry.k8s.io/sig-storage/csi-provisioner:v3.3.0\n    csiResizer: registry.k8s.io/sig-storage/csi-resizer:v1.6.0\n    csiSnapshotter: registry.k8s.io/sig-storage/csi-snapshotter:v6.1.0\n    csiNodeDriverRegistrar: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.2\n</code></pre> <p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728 K8s \u96c6\u7fa4\u4e2d\u521b\u5efa\u76f8\u5173\u8d44\u6e90\uff1a</p> <pre><code>unzip cephcsi-helmchart.zip\nhelm template ./cephcsi-helmchart -n cephfs -o ./deploy.yaml\nkubectl create ns cephfs\nkubectl create -n cephfs -f ./deploy.yaml\nkubectl get pod -n cephfs -w\n</code></pre>"},{"location":"k8s.html#\u521b\u5efa-pvc-\u5e76\u7ed1\u5b9a-pod","title":"\u521b\u5efa PVC \u5e76\u7ed1\u5b9a Pod","text":"<p>\u5728 PVC spec \u4e2d\u6307\u5b9a StorageClass \u540d\u79f0\u5373\u53ef\u521b\u5efa\u57fa\u4e8e Ceph \u5b58\u50a8\u96c6\u7fa4\u7684 PVC\u3002\u4f8b\u5982\uff0c\u914d\u7f6e Ceph CSI \u4e00\u8282\u4e2d\u521b\u5efa\u4e86\u540d\u4e3a sc-cephfs \u7684 Storage Class\uff0c\u901a\u8fc7\u5982\u4e0b YAML \u521b\u5efa PVC\uff1a</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cephfs-pvc\nspec:\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: sc-cephfs\n</code></pre> <p>\u7136\u540e\u521b\u5efa Pod \u7ed1\u5b9a\u8be5 PVC\uff1a</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n name: cephfs-demo-pod\nspec:\n containers:\n   - name: web-server\n     image: nginx:latest\n     resources:\n       limits:\n         cpu: 100m\n         memory: 200Mi\n     volumeMounts:\n       - name: cephfs-pvc\n         mountPath: /var/lib/www\n volumes:\n   - name: cephfs-pvc\n     persistentVolumeClaim:\n       claimName: cephfs-pvc\n</code></pre>"},{"location":"k8s.html#\u67e5\u770b-pvc-\u521b\u5efa\u7ed1\u5b9a\u5931\u8d25\u539f\u56e0","title":"\u67e5\u770b PVC \u521b\u5efa/\u7ed1\u5b9a\u5931\u8d25\u539f\u56e0","text":"<p>\u5982\u679c\u521b\u5efa PVC \u548c Pod \u540e\uff0cPod \u4e00\u76f4\u65e0\u6cd5\u6b63\u5e38\u8fd0\u884c\uff0c\u8bf7\u6309\u7167\u4ee5\u4e0b\u6b65\u9aa4\u67e5\u627e\u539f\u56e0\uff1a</p> <p>1. \u67e5\u770b PVC \u662f\u5426\u521b\u5efa\u6210\u529f</p> <p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u83b7\u53d6 PVC \u4fe1\u606f\uff1a</p> <pre><code>kubectl get pvc cephfs-pvc\n</code></pre> <p>\u5982\u679c STATUS \u5b57\u6bb5\u662f Bound\uff0c\u5219\u8f6c\u81f3\u7b2c 2 \u6b65\u3002</p> <p>\u5982\u679c STATUS \u5b57\u6bb5\u662f Pending\uff0c\u8bf4\u660e PVC \u6ca1\u6709 provision \u6210\u529f\uff0c\u8bf7\u67e5\u770b Ceph CSI Provisioner Pod \u7684\u65e5\u5fd7\u5bfb\u627e\u539f\u56e0\u3002</p> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u83b7\u53d6 Ceph CSI Provisioner Pod \u540d\u79f0\uff1a</p> <pre><code>$ kubectl get pod -n cephfs -l app=csi-cephfsplugin-provisioner\nNAME                                            READY   STATUS    RESTARTS   AGE\ncsi-cephfsplugin-provisioner-5b77dc5fff-56mr9   6/6     Running   0          6d21h\ncsi-cephfsplugin-provisioner-5b77dc5fff-rcpbg   6/6     Running   0          6d21h\ncsi-cephfsplugin-provisioner-5b77dc5fff-tkmsh   6/6     Running   0          6d21h\n</code></pre> <p>\u5176\u4e2d\u53ea\u6709\u4e00\u4e2a Pod \u662f\u4e3b\u8981\u7684\u5de5\u4f5c Pod\uff0c\u53e6\u5916\u4e24\u4e2a\u5904\u4e8e\u5f85\u547d\u72b6\u6001\u3002\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u5176\u65e5\u5fd7\uff1a</p> <pre><code>kubectl logs -n cephfs csi-cephfsplugin-provisioner-5b77dc5fff-56mr9 csi-provisioner\n</code></pre> <p>2. \u67e5\u770b PVC \u4e0e Pod \u662f\u5426\u7ed1\u5b9a\u6210\u529f</p> <p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u83b7\u53d6 Pod \u4fe1\u606f\uff1a</p> <pre><code>kubectl describe pod cephfs-demo-pod\n</code></pre> <p>\u8f93\u51fa\u7ed3\u679c\u4e2d\u7684 Events \u5b57\u6bb5\u4f1a\u663e\u793a PVC \u7ed1\u5b9a\u4e0d\u6210\u529f\u7684\u539f\u56e0\u3002</p> <p>\u53e6\u5916\uff0c\u67e5\u770b Ceph CSI Plugin Pod \u65e5\u5fd7\u4e5f\u6709\u52a9\u4e8e\u67e5\u627e\u539f\u56e0\u3002</p> <p>\u9996\u5148\uff0c\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u6240\u521b\u5efa\u7684 Pod \u4f4d\u4e8e\u54ea\u4e2a\u8282\u70b9\u4e0a\uff1a</p> <pre><code>$ kubectl get pod -o wide\nNAME              READY   STATUS    RESTARTS   AGE     IP               NODE\ncephfs-demo-pod   1/1     Running   0          6d22h   10.233.106.203   nc13\n</code></pre> <p>\u7136\u540e\uff0c\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u627e\u5230\u4f4d\u4e8e\u540c\u4e00\u4e2a\u8282\u70b9\u4e0a\u7684 Ceph CSI Plugin Pod\uff1a</p> <pre><code>$ kubectl get pod -n cephfs -l app=csi-cephfsplugin -o wide\nNAME                     READY   STATUS    RESTARTS   AGE    IP             NODE\ncsi-cephfsplugin-55nc4   3/3     Running   0          3d1h   100.64.4.74    nc14\ncsi-cephfsplugin-dd99m   3/3     Running   0          3d1h   100.64.4.199   nuc\ncsi-cephfsplugin-jcxv6   3/3     Running   0          3d1h   100.64.4.71    nc11\ncsi-cephfsplugin-jf4mn   3/3     Running   0          3d1h   100.64.4.209   acer\ncsi-cephfsplugin-jhffm   3/3     Running   0          3d1h   10.0.0.2    e01\ncsi-cephfsplugin-mxjh2   3/3     Running   0          3d1h   100.64.4.73    nc13\ncsi-cephfsplugin-rdbbp   3/3     Running   0          3d1h   100.64.4.72    nc12\ncsi-cephfsplugin-vql6n   3/3     Running   0          3d1h   100.64.4.132   kylin\n</code></pre> <p>\u6700\u540e\uff0c\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u8be5 Ceph CSI Plugin Pod \u7684\u65e5\u5fd7\uff1a</p> <pre><code>kubectl logs -n cephfs csi-cephfsplugin-mxjh2 csi-cephfsplugin\n</code></pre> <p>\u5982\u679c\u4ecd\u7136\u65e0\u6cd5\u627e\u5230\u95ee\u9898\u539f\u56e0\uff0c\u53ef\u5c1d\u8bd5\u91cd\u542f\u8be5 Ceph CSI Plugin Pod\uff1a</p> <pre><code>kubectl delete -n cephfs csi-cephfsplugin-mxjh2\n</code></pre> <p>\u5982\u679c\u4e0a\u8ff0\u5220\u9664\u547d\u4ee4\u5361\u4f4f\uff0c\u53ef\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5f3a\u884c\u5220\u9664\u8be5 Ceph CSI Plugin Pod\uff1a</p> <pre><code>kubectl delete -n cephfs csi-cephfsplugin-mxjh2 --force\n</code></pre>"},{"location":"k8s.html#pvc-\u5907\u4efd","title":"PVC \u5907\u4efd","text":"<p>\u4e3a\u4e86\u652f\u6301 PVC \u5907\u4efd\uff0c\u9996\u5148\u9700\u8981\u5728 K8s \u4e2d\u542f\u7528 Volume Snapshot \u529f\u80fd\uff1b\u5176\u6b21\uff0c\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u7684 PVC Provisioner \u521b\u5efa VolumeSnapshotClass\u3002</p> <p>Ceph CSI \u63d0\u4f9b\u4e86\u5bf9\u5e94\u7684 VolumeSnapshotClass \u8d44\u6e90\uff0c\u5176 YAML \u5982\u4e0b\uff08\u6765\u6e90\uff09\uff1a</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n name: csi-cephfsplugin-snapclass\ndriver: cephfs.csi.ceph.com\nparameters:\n # String representing a Ceph cluster to provision storage snapshot from.\n # Should be unique across all Ceph clusters in use for provisioning,\n # cannot be greater than 36 bytes in length, and should remain immutable for\n # the lifetime of the StorageClass in use.\n # Ensure to create an entry in the configmap named ceph-csi-config, based on\n # csi-config-map-sample.yaml, to accompany the string chosen to\n # represent the Ceph cluster in clusterID below\n clusterID: 47f20cbc-914f-24ed-93dc-9f2800951ba2\n\n # Prefix to use for naming CephFS snapshots.\n # If omitted, defaults to \"csi-snap-\".\n # snapshotNamePrefix: \"foo-bar-\"\n\n csi.storage.k8s.io/snapshotter-secret-name: csi-cephfs-secret\n csi.storage.k8s.io/snapshotter-secret-namespace: default\ndeletionPolicy: Delete\n</code></pre> <p>\u4e0e\u914d\u7f6e Ceph CSI \u4e00\u8282\u7c7b\u4f3c\uff0cYAML \u4e2d\u9700\u8981\u586b\u5165 Ceph File System \u7684 fsid\u3002</p> <p>\u9488\u5bf9\u521b\u5efa PVC \u5e76\u7ed1\u5b9a Pod \u4e00\u8282\u4e2d\u6240\u521b\u5efa\u7684\u540d\u4e3a cephfs-pvc \u7684 PVC\uff0c\u6211\u4eec\u521b\u5efa\u4e00\u4e2a VolumeSnapshot \u6765\u5bf9\u5176\u8fdb\u884c\u5907\u4efd\uff0cYAML \u5982\u4e0b\uff08\u6765\u6e90\uff09\uff1a</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n name: cephfs-pvc-snapshot\nspec:\n volumeSnapshotClassName: csi-cephfsplugin-snapclass\n source:\n   persistentVolumeClaimName: cephfs-pvc\n</code></pre> <p>\u67e5\u770b\u6240\u521b\u5efa\u7684\u5907\u4efd\uff1a</p> <pre><code>kubectl describe volumesnapshot cephfs-pvc-snapshot\n</code></pre> <p>\u57fa\u4e8e\u8be5\u5907\u4efd\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 PVC\uff1a</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n name: cephfs-pvc-restore\nspec:\n storageClassName: tsz-cephfs\n dataSource:\n   name: cephfs-pvc-snapshot\n   kind: VolumeSnapshot\n   apiGroup: snapshot.storage.k8s.io\n accessModes:\n   - ReadWriteMany\n resources:\n   requests:\n     storage: 1Gi\n</code></pre> <p>\u7136\u540e\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 Pod \u7ed1\u5b9a\u8be5 PVC\uff1a</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n name: cephfs-restore-demo-pod\nspec:\n containers:\n   - name: web-server\n     image: nginx:latest\n     resources:\n       limits:\n         cpu: 100m\n         memory: 200Mi\n     volumeMounts:\n       - name: cephfs-pvc-restore\n         mountPath: /var/lib/www\n volumes:\n   - name: cephfs-pvc-restore\n     persistentVolumeClaim:\n       claimName: cephfs-pvc-restore\n</code></pre> <p>\u6700\u540e\uff0c\u8fdb\u5165 Pod \u67e5\u770b PVC \u4e2d\u7684\u5185\u5bb9\u662f\u5426\u4e0e\u4e4b\u524d\u76f8\u540c\uff1a</p> <pre><code>kubectl exec -it cephfs-restore-demo-pod -- /bin/bash\nls /var/lib/www\n</code></pre>"},{"location":"k8s.html#\u5728-k8s-\u4e2d\u542f\u7528-volume-snapshot-\u529f\u80fd","title":"\u5728 K8s \u4e2d\u542f\u7528 Volume Snapshot \u529f\u80fd","text":"<p>\u6839\u636e\u5b98\u65b9\u535a\u5ba2\uff0cVolume Snapshot \u7279\u6027\u4ece K8s 1.20 \u7248\u672c\u5f00\u59cb GA\u3002</p> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b K8s \u7248\u672c\uff1a</p> <pre><code>kubectl version\n</code></pre> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b K8s \u4e2d\u662f\u5426\u5df2\u7ecf\u5b89\u88c5 Volume Snapshot \u76f8\u5173\u7ec4\u4ef6\uff1a</p> <pre><code>kubectl get deploy -n kube-system snapshot-controller\nkubectl api-resouces | grep volumesnapshot\n</code></pre> <p>\u5982\u679c\u5c1a\u672a\u5b89\u88c5\uff0c\u53ef\u4ee5\u6839\u636e\u5b98\u65b9\u6587\u6863\uff0c\u624b\u52a8\u5b89\u88c5 CRD\u3001controller\u3001webhook \u7b49\u7ec4\u4ef6\uff1a</p> <pre><code>git clone https://github.com/kubernetes-csi/external-snapshotter.git\ncd external-snapshotter\nkubectl kustomize client/config/crd | kubectl create -f -\nkubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f -\n</code></pre>"},{"location":"k8s.html#\u914d\u989d\u7ba1\u7406","title":"\u914d\u989d\u7ba1\u7406","text":"<p>PVC \u80fd\u591f\u4f7f\u7528\u7684\u6700\u5927\u5b58\u50a8\u7a7a\u95f4\u7531\u5176 spec \u6307\u5b9a\uff0cK8s \u96c6\u7fa4\u4e2d\u6240\u6709\u57fa\u4e8e Ceph \u7684 PVC \u603b\u5171\u80fd\u591f\u4f7f\u7528\u7684\u6700\u5927\u5b58\u50a8\u7a7a\u95f4\u53ef\u901a\u8fc7 Ceph File System \u7684\u5e95\u5c42 Pool \u6765\u9650\u5236\u3002</p> <p>\u5728\u4efb\u610f\u7ba1\u7406\u8282\u70b9\u4e0a\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5217\u4e3e\u6240\u6709 Pool\uff1a</p> <pre><code>$ sudo ceph osd pool ls\n.mgr\ncephfs.demo-fs.meta\ncephfs.demo-fs.data\n</code></pre> <p>\u5176\u4e2d\uff0c\u540d\u4e3a <code>cephfs.demo-fs.data</code> \u7684 Pool \u5c31\u662f\u540d\u4e3a demo-fs \u7684 Ceph File System \u5b58\u50a8\u6570\u636e\u7684 Pool\uff0c\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u9650\u5236\u8be5 Pool \u80fd\u591f\u4f7f\u7528\u7684\u6700\u5927\u5b58\u50a8\u7a7a\u95f4\uff1a</p> <pre><code>sudo ceph osd pool set-quota cephfs.demo-fs.data max_bytes 10000000\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Set Pool Quotas</li> <li>Ceph - Quotas</li> </ul>"},{"location":"operations.html","title":"\u96c6\u7fa4\u8fd0\u7ef4","text":""},{"location":"operations.html#\u67e5\u770b\u96c6\u7fa4\u72b6\u6001","title":"\u67e5\u770b\u96c6\u7fa4\u72b6\u6001","text":"<p>Ceph \u63d0\u4f9b\u4e86\u5b8c\u5584\u7684 Web UI \u6765\u67e5\u770b\u548c\u64cd\u4f5c\u96c6\u7fa4\u4e2d\u7684\u5404\u9879\u670d\u52a1\u3002\u9996\u5148\uff0c\u4f7f\u7528\u7ba1\u7406\u5458\u7684\u7528\u6237\u540d\u548c\u5bc6\u7801\u767b\u5f55 Ceph Web UI\u3002</p> <p>\u70b9\u51fb\u5de6\u4fa7\u5bfc\u822a\u680f\u7684 Dashboard\uff0c\u67e5\u770b\u96c6\u7fa4\u72b6\u6001\u603b\u89c8\u3002</p> <p>\u70b9\u51fb\u5de6\u4fa7\u5bfc\u822a\u680f\u7684 Cluster &gt; OSDs\uff0c\u67e5\u770b\u5404\u4e2a\u78c1\u76d8\u7684\u4f7f\u7528\u60c5\u51b5\u3002</p> <p>\u70b9\u51fb\u5de6\u4fa7\u5bfc\u822a\u680f\u7684 Cluster &gt; OSDs\uff0c\u7136\u540e\u70b9\u51fb\u4e0a\u4fa7\u7684 Overall Performance\uff0c\u67e5\u770b\u78c1\u76d8\u4f7f\u7528\u60c5\u51b5\u7684\u53ef\u89c6\u5316\u7edf\u8ba1\u6570\u636e\u3002</p> <p>\u70b9\u51fb\u5de6\u4fa7\u5bfc\u822a\u680f\u7684 Cluster &gt; Logs\uff0c\u67e5\u770b\u96c6\u7fa4\u65e5\u5fd7\u3002</p> <p>\u53e6\u5916\uff0c\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7 SSH \u767b\u5f55 Ceph \u96c6\u7fa4\u7684\u7ba1\u7406\u8282\u70b9\uff0c\u4f7f\u7528\u547d\u4ee4\u884c\u6765\u67e5\u770b\u3002</p> <p>\u4ec0\u4e48\u662f\u7ba1\u7406\u8282\u70b9</p> <p>\u6dfb\u52a0\u4e86 _admin \u6807\u7b7e\u7684\u8282\u70b9\u662f\u7ba1\u7406\u8282\u70b9\uff0c\u62e5\u6709\u7ba1\u7406\u6574\u4e2a\u96c6\u7fa4\u7684\u6743\u9650\u3002\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5c06\u4e00\u4e2a\u8282\u70b9\u8bbe\u4e3a\u7ba1\u7406\u8282\u70b9\uff1a</p> <pre><code>sudo ceph orch host add &lt;host-name&gt; &lt;host-ip&gt; --labels _admin\n</code></pre> <p>\u67e5\u770b\u96c6\u7fa4\u72b6\u6001\u603b\u89c8\uff1a</p> <pre><code>sudo ceph status\nsudo ceph -s\nsudo ceph health detail\n</code></pre> <p>\u67e5\u770b\u672c\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u5404\u9879\u670d\u52a1\u72b6\u6001\uff1a</p> <pre><code>sudo systemctl status ceph\\*.service ceph\\*.target\n</code></pre> <p>\u67e5\u770b\u672c\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u67d0\u9879\u670d\u52a1\u7684\u8be6\u7ec6\u65e5\u5fd7\uff1a</p> <pre><code>sudo journalctl -u &lt;service-name&gt;\n</code></pre> <p>\u4f8b\u5982\uff1a</p> <pre><code>sudo journalctl -u ceph-osd@0.service\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Operating a Cluster</li> </ul>"},{"location":"operations.html#\u7ba1\u7406-alert","title":"\u7ba1\u7406 alert","text":"<p>\u5728 Ceph Dashboard \u4e2d\uff0c\u70b9\u51fb\u5de6\u4fa7\u5bfc\u822a\u680f\u7684 Cluster &gt; Monitoring\uff0c\u53ef\u4ee5\u67e5\u770b\u5f53\u524d\u96c6\u7fa4\u4e2d\u7684\u8b66\u544a\u4fe1\u606f\u3002</p> <p>\u5982\u679c\u786e\u8ba4\u8b66\u544a\u4fe1\u606f\u65e0\u5173\u7d27\u8981\uff0c\u60a8\u53ef\u4ee5\u9759\u9ed8\u8b66\u544a\u4fe1\u606f\u3002\u9996\u5148\u5728\u8868\u683c\u4e2d\u70b9\u51fb\u6240\u8981\u9759\u9ed8\u7684\u884c\uff0c\u7136\u540e\u70b9\u51fb\u5de6\u4e0a\u89d2\u7684 Create Silence \u6309\u94ae\uff0c\u586b\u5199\u9759\u9ed8\u65f6\u957f\u548c\u7b5b\u9009\u6761\u4ef6\u7b49\u914d\u7f6e\u5373\u53ef\u3002</p> <p>\u53e6\u5916\uff0c\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7\u547d\u4ee4\u884c\u6765\u9759\u9ed8\u8b66\u544a\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u5982\u679c\u901a\u8fc7\u547d\u4ee4\u884c\u67e5\u770b\u96c6\u7fa4\u72b6\u6001\u53d1\u73b0\u6709\u5982\u4e0b\u8b66\u544a\uff1a</p> <pre><code>$ ceph health detail\n[WRN] MDS_SLOW_METADATA_IO: 1 MDSs report slow metadata IOs\n</code></pre> <p>\u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u9759\u9ed8\u8be5\u8b66\u544a\uff1a</p> <pre><code>ceph health mute MDS_SLOW_METADATA_IO\n</code></pre> <p>\u6216\u5728\u4e00\u5b9a\u65f6\u95f4\u5185\u9759\u9ed8\u8b66\u544a\uff1a</p> <pre><code>ceph health mute MDS_SLOW_METADATA_IO 1h\n</code></pre> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u53d6\u6d88\u9759\u9ed8\uff1a</p> <pre><code>ceph health unmute MDS_SLOW_METADATA_IO\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Muting Health Checks</li> </ul>"},{"location":"operations.html#\u67e5\u770b-daemon-\u72b6\u6001","title":"\u67e5\u770b daemon \u72b6\u6001","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u96c6\u7fa4\u4e2d\u8fd0\u884c\u7684\u6240\u6709 daemon\uff1a</p> <pre><code>sudo ceph orch ls\n\nsudo ceph orch ps \n\nsudo ceph orch ls --export &gt; cluster.yaml\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Service Management</li> </ul>"},{"location":"operations.html#\u83b7\u53d6-daemon-\u5730\u5740","title":"\u83b7\u53d6 daemon \u5730\u5740","text":"<p>\u4e3a\u4e86\u83b7\u53d6\u67d0\u4e2a daemon \u7684\u5730\u5740\uff0c\u4f8b\u5982 prometheus\u3001grafana\u3001alert manager \u7b49\uff0c\u9996\u5148\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b daemon \u6240\u5728\u7684\u8282\u70b9\u4e0e\u7aef\u53e3\uff1a</p> <pre><code>$ sudo ceph orch ps\nNAME                           HOST   PORTS        STATUS        REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID\nalertmanager.host1             host1  *:9093,9094  running (5w)     4m ago   8w    54.3M        -           ba2b418f427c  10679fbf3b3d\ngrafana.host1                  host1  *:3000       running (5w)     4m ago   8w     149M        -  8.3.5    dad864ee21e9  526bb2c28317\nprometheus.host1               host1  *:9095       running (5w)     4m ago   8w     213M        -           514e6a882f6e  8579c7c74cc5\n</code></pre> <p>\u53ef\u4ee5\u770b\u5230\uff0cprometheus\u3001grafana\u3001alert manager \u5747\u8fd0\u884c\u5728 host1 \u8282\u70b9\u4e0a\uff0c\u5e76\u5206\u522b\u4f7f\u7528 9095\u30013000\u30019093 \u7aef\u53e3\u3002</p> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b host1 \u8282\u70b9\u7684 ip\uff1a</p> <pre><code>$ sudo \u200b\u200bceph orch host ls\nHOST    ADDR          LABELS  STATUS\nhost1   10.1.2.3      _admin\n</code></pre> <p>\u56e0\u6b64\uff1a</p> <ul> <li>prometheus \u7684\u5730\u5740\u4e3a <code>http://10.1.2.3:9095</code></li> <li>grafana \u7684\u5730\u5740\u4e3a <code>http://10.1.2.3:3000</code></li> <li>alert manager \u7684\u5730\u5740\u4e3a <code>http://10.1.2.3:9093</code></li> </ul>"},{"location":"operations.html#\u8c03\u6574-daemon-\u914d\u7f6e","title":"\u8c03\u6574 daemon \u914d\u7f6e","text":"<p>Ceph \u652f\u6301\u4ee5 yaml \u914d\u7f6e\u6587\u4ef6\u7684\u5f62\u5f0f\u6307\u5b9a daemon \u7684\u8be6\u7ec6 spec\u3002</p> <p>\u4ee5 rgw daemon \u4e3a\u4f8b\uff0c\u5bfc\u51fa\u914d\u7f6e\u6587\u4ef6\uff1a</p> <pre><code>sudo ceph orch ls --service_type rgw --export &gt; rgw.yaml\n</code></pre> <p>rgw.yaml \u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>service_type: rgw\nservice_id: cephs3\nservice_name: rgw.cephs3\nplacement:\n  hosts:\n  - host1\n</code></pre> <p>\u4fee\u6539 rgw.yaml \u4e3a\u4ee5\u4e0b\u5185\u5bb9\uff1a</p> <pre><code>service_type: rgw\nservice_id: cephs3\nservice_name: rgw.cephs3\nplacement:\n  count: 2\n  hosts:\n  - host1\n  - host2\nnetworks:\n- 172.0.0.0/24\nspec:\n  rgw_frontend_port: 8081\n</code></pre> <p>\u5176\u4e2d\uff1a</p> <ul> <li>\u914d\u7f6e rgw daemon \u4e3a 2 \u4e2a\uff0chost1 \u8282\u70b9\u3001host2 \u8282\u70b9\u5404\u4e00\u4e2a</li> <li>\u914d\u7f6e rgw daemon \u4f7f\u7528 172.0.0.0/24 \u7f51\u7edc</li> <li>\u914d\u7f6e rgw daemon \u4f7f\u7528 8081 \u7aef\u53e3</li> </ul> <p>\u5e94\u7528\u8be5\u914d\u7f6e\u6587\u4ef6\uff1a</p> <pre><code>sudo ceph orch apply -i ./rgw.yaml\n</code></pre>"},{"location":"operations.html#\u79fb\u9664-daemon","title":"\u79fb\u9664 daemon","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u79fb\u9664\u67d0\u4e2a daemon\uff1a</p> <pre><code>sudo ceph orch daemon rm &lt;daemon-name&gt;\n</code></pre> <p>\u4f8b\u5982\uff1a</p> <pre><code>sudo ceph orch daemon rm mgr.ds03.obymbg\n</code></pre>"},{"location":"operations.html#\u79fb\u9664-service","title":"\u79fb\u9664 service","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u79fb\u9664\u67d0\u4e2a service\uff1a</p> <pre><code>sudo ceph orch rm &lt;service-name&gt;\n</code></pre> <p>\u4f8b\u5982\uff1a</p> <pre><code>sudo ceph orch rm mds.k8s\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Removing a Service</li> </ul>"},{"location":"operations.html#\u67e5\u770b-mon-ip","title":"\u67e5\u770b mon ip","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b mon ip\uff1a</p> <pre><code>sudo ceph mon dump\n</code></pre>"},{"location":"operations.html#\u521b\u5efa-mon","title":"\u521b\u5efa mon","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cCeph \u96c6\u7fa4\u81ea\u52a8\u7ba1\u7406 mon daemon\uff0c\u4e00\u822c\u5728 5 \u4e2a\u4e0d\u540c\u7684\u8282\u70b9\u4e0a\u90e8\u7f72 5 \u4e2a daemon\uff0c\u5360\u7528\u8282\u70b9\u7684 3300 \u548c 6789 \u7aef\u53e3\u3002</p> <p>\u5982\u679c\u9700\u8981\u624b\u52a8\u7ba1\u7406 mon\uff0c\u9996\u5148\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>sudo ceph orch apply mon --unmanaged\n</code></pre> <p>\u7136\u540e\u624b\u52a8\u521b\u5efa mon\uff1a</p> <pre><code>sudo ceph orch daemon add mon --placement=\"myhost:[v2:1.2.3.4:3300,v1:1.2.3.4:6789]=name\"\n</code></pre> <p>\u5176\u4e2d\uff0c<code>[v2:1.2.3.4:3300,v1:1.2.3.4:6789]</code> \u662f mon \u7684\u7f51\u7edc\u5730\u5740\uff0c<code>=name</code> \u7528\u4e8e\u6307\u5b9a\u6240\u521b\u5efa\u7684 mon \u540d\u79f0\u3002</p> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - MON Service</li> <li>Ceph - Explicit Placements</li> </ul>"},{"location":"operations.html#\u67e5\u770b-crash-\u5386\u53f2","title":"\u67e5\u770b crash \u5386\u53f2","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u96c6\u7fa4\u4e2d\u66fe\u7ecf\u53d1\u751f\u8fc7\u7684 crash \u4e8b\u4ef6\uff1a</p> <pre><code>sudo ceph crash ls\n</code></pre> <p>\u67e5\u770b\u67d0\u4e2a crash \u4e8b\u4ef6\u7684\u8be6\u7ec6\u4fe1\u606f\uff1a</p> <pre><code>sudo ceph crash info &lt;crash-id&gt;\n</code></pre> <p>\u5c06 crash \u4e8b\u4ef6\u5f52\u6863\uff0c\u4ee5\u514d\u51fa\u73b0\u5728 ceph status \u7684\u8f93\u51fa\u4e2d\uff1a</p> <pre><code>sudo ceph crash archive-all\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Crash Module</li> </ul>"},{"location":"operations.html#\u67e5\u770b\u8282\u70b9","title":"\u67e5\u770b\u8282\u70b9","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u96c6\u7fa4\u4e2d\u7684\u6240\u6709\u8282\u70b9\uff1a</p> <pre><code>sudo ceph orch host ls\n</code></pre>"},{"location":"operations.html#\u6dfb\u52a0\u8282\u70b9","title":"\u6dfb\u52a0\u8282\u70b9","text":"<p>\u89c1\u96c6\u7fa4\u5b89\u88c5 - \u6dfb\u52a0\u8282\u70b9\u3002</p>"},{"location":"operations.html#\u91cd\u542f\u8282\u70b9","title":"\u91cd\u542f\u8282\u70b9","text":"<p>\u901a\u8fc7 SSH \u767b\u5f55 Ceph \u96c6\u7fa4\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\uff0c\u53ef\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u91cd\u542f\u672c\u8282\u70b9\u4e0a\u7684 Ceph \u670d\u52a1\uff1a</p> <pre><code>sudo systemctl stop ceph\\*.service ceph\\*.target\nsudo systemctl start ceph.target \n</code></pre>"},{"location":"operations.html#\u79fb\u9664\u8282\u70b9","title":"\u79fb\u9664\u8282\u70b9","text":"<p>\u5047\u8bbe\u5c06\u88ab\u79fb\u9664\u7684\u8282\u70b9\u540d\u79f0\u4e3a host1\uff0c\u4ee5\u4e0b\u547d\u4ee4\u5747\u9700\u8981\u5728 Ceph \u96c6\u7fa4\u7684\u5176\u4ed6\u7ba1\u7406\u8282\u70b9\u4e0a\u8fd0\u884c\u3002</p> <p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u79fb\u9664 host1\uff1a</p> <pre><code>sudo ceph orch host ls   \nsudo ceph orch host drain host1\nsudo ceph mon remove host1\n</code></pre> <p>\u89c2\u5bdf OSD \u7684\u79fb\u9664\u60c5\u51b5\uff0c\u76f4\u5230\u5176\u62a5\u544a \u201cNo OSD remove/replace operations reported\u201d\uff1a</p> <pre><code>sudo ceph orch osd rm status\n</code></pre> <p>\u89c2\u5bdf Daemon \u7684\u79fb\u9664\u60c5\u51b5\uff08\u8fd9\u53ef\u80fd\u4f1a\u9700\u8981\u8f83\u957f\u7684\u65f6\u95f4\uff09\uff0c\u76f4\u5230\u5176\u62a5\u544a \u201cNo daemons reported\u201d\uff1a</p> <pre><code>sudo ceph orch ps host1\n</code></pre> <p>\u6240\u6709 Daemon \u90fd\u88ab\u79fb\u9664\u540e\u624d\u80fd\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5220\u9664\u8282\u70b9 host1\uff1a</p> <pre><code>sudo ceph orch host rm host1\nsudo ceph orch host ls\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Removing Hosts </li> </ul>"},{"location":"operations.html#\u67e5\u770b\u5b58\u50a8\u8bbe\u5907","title":"\u67e5\u770b\u5b58\u50a8\u8bbe\u5907","text":"<p>\u67e5\u770b\u8282\u70b9\u4e0a\u7684\u6240\u6709\u5b58\u50a8\u8bbe\u5907\uff1a</p> <pre><code>lsblk\n</code></pre> <p>\u67e5\u770b\u96c6\u7fa4\u4e2d Ceph \u80fd\u770b\u5230\u7684\u6240\u6709\u5b58\u50a8\u8bbe\u5907\uff1a</p> <pre><code>sudo ceph orch device ls\n</code></pre> <p>\u67e5\u770b\u96c6\u7fa4\u4e2d\u8fd0\u884c\u7684\u6240\u6709 OSD\uff1a</p> <pre><code>sudo ceph osd status\n</code></pre>"},{"location":"operations.html#\u6dfb\u52a0\u5b58\u50a8\u8bbe\u5907","title":"\u6dfb\u52a0\u5b58\u50a8\u8bbe\u5907","text":"<p>\u89c1\u96c6\u7fa4\u5b89\u88c5 - \u6dfb\u52a0\u5b58\u50a8\u8bbe\u5907\u3002</p>"},{"location":"operations.html#\u68c0\u67e5\u5b58\u50a8\u8bbe\u5907","title":"\u68c0\u67e5\u5b58\u50a8\u8bbe\u5907","text":"<p>\u5982\u679c\u67d0\u4e2a\u5b58\u50a8\u8bbe\u5907\u51fa\u73b0\u5f02\u5e38\uff0c\u8bfb\u5199\u901f\u5ea6\u8f83\u6162\uff0c\u53ef\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u68c0\u67e5\uff1a</p> <pre><code>hdparm -Tt &lt;device-path&gt;\n</code></pre> <p>\u4f8b\u5982\uff1a</p> <pre><code>hdparm -Tt /dev/sdb\n</code></pre>"},{"location":"operations.html#\u79fb\u9664\u5b58\u50a8\u8bbe\u5907","title":"\u79fb\u9664\u5b58\u50a8\u8bbe\u5907","text":"<p>\u5047\u8bbe\u5c06\u88ab\u79fb\u9664\u7684\u78c1\u76d8\u4e0a\u8fd0\u884c\u7684 OSD \u7f16\u53f7\u4e3a 0\uff08\u53ef\u901a\u8fc7 Web UI \u6216 sudo ceph osd status \u547d\u4ee4\u83b7\u53d6\uff09\u3002</p> <p>\u5728\u78c1\u76d8\u6240\u5c5e\u8282\u70b9\u4e0a\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u79fb\u9664 OSD\uff0c\u5e76\u81ea\u52a8\u5316\u5730\u5c06\u8be5\u78c1\u76d8\u4e2d\u5b58\u50a8\u7684\u6570\u636e\u79fb\u52a8\u5230\u5176\u4ed6\u78c1\u76d8\u4e2d\uff1a</p> <pre><code>sudo ceph orch osd rm 0\n</code></pre> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b OSD \u79fb\u9664\u8fdb\u5c55\uff1a</p> <pre><code>sudo ceph orch osd rm status\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Remove an OSD </li> </ul>"},{"location":"operations.html#\u5b58\u50a8\u8bbe\u5907\u6545\u969c\u6062\u590d","title":"\u5b58\u50a8\u8bbe\u5907\u6545\u969c\u6062\u590d","text":"<p>\u5982\u679c\u67d0\u4e2a\u5b58\u50a8\u8bbe\u5907\u53d1\u751f\u6545\u969c\uff0c\u5176\u4e2d\u7684\u6570\u636e\u65e0\u6cd5\u8bfb\u53d6\uff0c\u8be5\u5b58\u50a8\u8bbe\u5907\u5bf9\u5e94\u7684 OSD Daemon \u5728\u4e00\u6bb5\u65f6\u95f4\u540e\u4f1a\u81ea\u52a8\u53d1\u73b0\u6545\u969c\u5e76\u663e\u793a\u5728\u96c6\u7fa4\u72b6\u6001\u603b\u89c8\u4e2d\u3002\u6b64\u65f6\uff0c\u7ba1\u7406\u5458\u5e94\u5f53\u6309\u7167\u79fb\u9664\u5b58\u50a8\u8bbe\u5907\u4e00\u8282\u4e2d\u7684\u6b65\u9aa4\u79fb\u9664\u6545\u969c\u8bbe\u5907\uff0c\u5e76\u5c06\u5176\u4ece\u7269\u7406\u673a\u5668\u4e2d\u62d4\u51fa\u3002</p> <p>\u7531\u4e8e Ceph \u4f7f\u7528\u4e86\u5197\u4f59\u5b58\u50a8\u673a\u5236\uff0c\u4e00\u4e2a\u5b58\u50a8\u8bbe\u5907\u7684\u635f\u574f\u5e76\u4e0d\u4f1a\u9020\u6210\u6570\u636e\u7684\u4e22\u5931\uff0cCeph \u5c06\u81ea\u52a8\u6062\u590d\u635f\u574f\u7684\u6570\u636e\u3002\u4e00\u4e2a Ceph \u96c6\u7fa4\u5177\u4f53\u80fd\u591f\u5bb9\u5fcd\u7684\u5b58\u50a8\u8bbe\u5907\u635f\u574f\u6570\u91cf\u6839\u636e\u914d\u7f6e\u7684\u4e0d\u540c\u6709\u6240\u533a\u522b\uff0c\u8be6\u89c1 Erasure Code \u4e00\u8282\u3002</p>"},{"location":"operations.html#\u67e5\u770b\u5b58\u50a8\u7a7a\u95f4","title":"\u67e5\u770b\u5b58\u50a8\u7a7a\u95f4","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u5b58\u50a8\u7a7a\u95f4\u7684\u6574\u4f53\u4f7f\u7528\u60c5\u51b5\uff1a</p> <pre><code>sudo ceph df\n</code></pre> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u6bcf\u4e2a osd \u7684\u5b58\u50a8\u7a7a\u95f4\u4f7f\u7528\u60c5\u51b5\uff1a</p> <pre><code>sudo ceph osd df\n</code></pre>"},{"location":"operations.html#\u91cd\u542f-mgr","title":"\u91cd\u542f mgr","text":"<p>mgr \u8d1f\u8d23\u96c6\u7fa4\u7684\u5916\u56f4\u76d1\u63a7\u548c\u7ba1\u7406\uff0cdashboard \u5373\u4e3a mgr \u7684\u4e00\u4e2a\u7ec4\u4ef6\u3002\u5982\u679c dashboard \u65e0\u6cd5\u8bbf\u95ee\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u91cd\u542f mgr\u3002</p> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5c06\u5f53\u524d\u6b63\u5728\u8fd0\u884c\u7684 mgr daemon \u6807\u8bb0\u4e3a\u5931\u8d25\uff0cceph \u4f1a\u81ea\u52a8\u91cd\u542f\u8be5 daemon \u5e76\u542f\u7528\u5019\u8865 daemon\uff1a</p> <pre><code>sudo ceph mgr fail\n</code></pre>"},{"location":"operations.html#\u914d\u7f6e-crushmap","title":"\u914d\u7f6e crushmap","text":"<p>crushmap \u7528\u4e8e\u6307\u5b9a\u6570\u636e\u5982\u4f55\u5b58\u653e\u5230\u5b58\u50a8\u8bbe\u5907\u4e2d\uff0c\u4f8b\u5982\u4ec5\u4f7f\u7528 hdd\u3001\u4ec5\u4f7f\u7528 ssd\u3001\u4ee5 replicate \u7684\u5f62\u5f0f\u3001\u4ee5 erasure code \u7684\u5f62\u5f0f\u3002</p> <p>\u9996\u5148\u5bfc\u51fa  crushmap\uff1a</p> <pre><code>sudo ceph osd getcrushmap -o comp_crush_map.cm\ncrushtool -d comp_crush_map.cm -o crush_map.cm\n</code></pre> <p>\u67e5\u770b crushmap\uff1a</p> <pre><code>$ cat crush_map.cm\n\n...\n# rules\nrule replicated_hdd {\n    id 0\n    type replicated\n    step take default class hdd\n    step chooseleaf firstn 0 type osd\n    step emit\n}\nrule replicated_ssd {\n    id 1\n    type replicated\n    step take default class ssd\n    step chooseleaf firstn 0 type osd\n    step emit\n}\nrule ecpool-k4-m2 {\n    id 2\n    type erasure\n    step set_chooseleaf_tries 5\n    step set_choose_tries 100\n    step take default class ssd\n    step choose indep 0 type osd\n    step emit\n}\n</code></pre> <p>\u4ece\u4e0a\u8ff0 crushmap \u793a\u4f8b\u53ef\u4ee5\u770b\u51fa\uff1a</p> <ul> <li><code>type replicated</code> \u8868\u793a crush rule \u662f replicate \u7c7b\u578b</li> <li><code>type erasure</code> \u8868\u793a crush rule \u662f erasure code \u7c7b\u578b</li> <li><code>step take default class hdd</code> \u8868\u793a crush rule \u4ec5\u4f7f\u7528 hdd</li> <li><code>step take default class ssd</code> \u8868\u793a crush rule \u4ec5\u4f7f\u7528 ssd</li> <li><code>step chooseleaf firstn 0 type osd</code> \u8868\u793a crush rule \u5728\u4e0d\u540c\u7684 osd \u4e2d\u5b58\u50a8\u6570\u636e\u7684\u591a\u4e2a\u5907\u4efd</li> </ul> <p>\u60a8\u53ef\u4ee5\u7f16\u8f91\u5e76\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528 crushmap\uff1a</p> <pre><code>crushtool -c crush_map.cm -o new_crush_map.cm\nsudo ceph osd setcrushmap -i new_crush_map.cm\n</code></pre> <p>\u7136\u540e\u5373\u53ef\u914d\u7f6e pool \u4f7f\u7528\u65b0\u7684 crush rule\uff0c\u53c2\u8003\u914d\u7f6e pool\u3002</p> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - CRUSH Maps</li> <li>Ceph - Manaully editing a CRUSH map</li> </ul>"},{"location":"operations.html#\u67e5\u770b-pool","title":"\u67e5\u770b pool","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u96c6\u7fa4\u4e2d\u6240\u6709\u7684 pool</p> <pre><code>sudo ceph osd lspools\n</code></pre>"},{"location":"operations.html#\u5220\u9664-pool","title":"\u5220\u9664 pool","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5220\u9664\u4e00\u4e2a pool\uff1a</p> <pre><code>sudo ceph osd pool rm &lt;pool-name&gt; &lt;pool-name&gt; --yes-i-really-really-mean-it\n</code></pre> <p>\u6ce8\u610f\uff1apool \u4e2d\u7684\u6570\u636e\u5c06\u88ab\u5f7b\u5e95\u5220\u9664\uff0c\u65e0\u6cd5\u6062\u590d\uff0c\u8bf7\u5c0f\u5fc3\u64cd\u4f5c\u3002</p>"},{"location":"operations.html#\u914d\u7f6e-pool","title":"\u914d\u7f6e pool","text":"<p>\u5982\u679c pool \u7684 crush rule \u662f replicate \u7c7b\u578b\uff0c\u53ef\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u8bbe\u7f6e replicate size\uff1a</p> <pre><code>sudo ceph osd pool set &lt;pool-name&gt; size &lt;size&gt; --yes-i-really-mean-it\nsudo ceph osd pool set &lt;pool-name&gt; min_size &lt;min-size&gt;\n</code></pre> <p>\u4e00\u822c\u60c5\u51b5\u4e0b\uff0creplicate pool \u7684 size \u4e3a 3\uff08\u8868\u793a\u6570\u636e\u4f1a\u88ab\u590d\u5236 3 \u4efd\uff09\uff0cmin_size \u4e3a 2\uff08\u8868\u793a\u6700\u5c11\u6709 2 \u4efd\u6570\u636e\u624d\u80fd\u6b63\u5e38\u5de5\u4f5c\uff09\u3002</p> <p>\u7279\u6b8a\u60c5\u51b5\u4e0b\uff0c\u4f8b\u5982\u5e95\u5c42\u5b58\u50a8\u5df2\u7ecf\u6709 RAID \u5bb9\u9519\u673a\u5236\uff0c\u53ef\u5c06 size \u548c min_size \u90fd\u8bbe\u4e3a 1\u3002</p> <p>\u5982\u679c\u96c6\u7fa4\u4e2d\u6709\u591a\u4e2a crush rule\uff0c\u53ef\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u4e3a pool \u6307\u5b9a crush rule\uff1a</p> <pre><code>sudo ceph osd pool set &lt;pool-name&gt; crush_rule &lt;crush-rule-name&gt;\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Pools</li> </ul>"},{"location":"operations.html#\u67e5\u770b-cephfs","title":"\u67e5\u770b cephfs","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u96c6\u7fa4\u4e2d\u6240\u6709\u7684 cephfs\uff1a</p> <pre><code>sudo ceph fs ls\n</code></pre>"},{"location":"operations.html#\u5220\u9664-cephfs","title":"\u5220\u9664 cephfs","text":"<p>\u4e3a\u4e86\u5220\u9664\u4e00\u4e2a cephfs\uff0c\u9996\u5148\u5c06\u5176\u6807\u8bb0\u4e3a\u4e0d\u53ef\u7528\uff1a</p> <pre><code>sudo ceph fs fail &lt;fs-name&gt;\n</code></pre> <p>\u7136\u540e\u786e\u8ba4\u5220\u9664\u8be5 cephfs\uff1a</p> <pre><code>sudo ceph fs rm &lt;fs-name&gt; --yes-i-really-mean-it\n</code></pre> <p>\u53c2\u8003\u5220\u9664 pool\uff0c\u5220\u9664\u8be5 cephfs \u4e0b\u5c5e\u6240\u6709\u7684 pool\u3002</p> <p>\u53c2\u8003\u79fb\u9664 daemon\uff0c\u79fb\u9664\u8be5 cephfs \u5bf9\u5e94\u7684\u6240\u6709 mds daemon\u3002</p>"},{"location":"operations.html#\u6302\u8f7d-cephfs","title":"\u6302\u8f7d cephfs","text":"<p>\u60a8\u53ef\u4ee5\u901a\u8fc7 mount \u547d\u4ee4\u5728\u4efb\u610f\u80fd\u591f\u8bbf\u95ee Ceph \u96c6\u7fa4\u7684\u673a\u5668\u4e0a\u6302\u8f7d cephfs\u3002</p> <p>\u9996\u5148\u5b89\u88c5 ceph \u5de5\u5177\uff1a</p> <pre><code>curl --silent --remote-name --location https://github.com/ceph/ceph/raw/quincy/src/cephadm/cephadm\nchmod +x cephadm\nsudo ./cephadm add-repo --release quincy\nsudo ./cephadm install ceph-common\n</code></pre> <p>\u7136\u540e\u914d\u7f6e\u8bbf\u95ee\u6743\u9650\uff1a</p> <pre><code>mkdir -p -m 755 /etc/ceph\nssh root@&lt;ceph-admin-node-ip&gt; \"sudo ceph config generate-minimal-conf\" | sudo tee /etc/ceph/ceph.conf\nchmod 644 /etc/ceph/ceph.conf\n\nssh root@&lt;ceph-admin-node-ip&gt; \"sudo ceph fs authorize &lt;fs-name&gt; client.foo / rw\" | sudo tee /etc/ceph/ceph.client.foo.keyring\nchmod 600 /etc/ceph/ceph.client.foo.keyring\n</code></pre> <p>\u6700\u540e\u6267\u884c mount \u547d\u4ee4\uff1a</p> <pre><code>mount -t ceph foo@&lt;cluster-id&gt;.&lt;fs-name&gt;=/ &lt;mount-path&gt; -o secret=&lt;client-keyring&gt;\n</code></pre> <p>\u4f8b\u5982\uff1a</p> <pre><code>mount -t ceph foo@f5e95c18-a869-11ed-ba9b-b95348587a43.k8s=/ /mnt/mycephfs -o secret=AQBFgglkj0N0CRAA0LHXD/QFerke3mZZ6W3UAw==\n</code></pre> <p>\u6ce8\u610f\uff1a\u4e0d\u8981\u76f4\u63a5\u5728 Ceph \u96c6\u7fa4\u7684\u8282\u70b9\u4e0a\u6302\u8f7d cephfs\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9020\u6210\u6b7b\u9501\u3002\u4f46\u5728\u5bb9\u5668\u4e2d\u6302\u8f7d cephfs \u662f\u53ef\u4ee5\u7684\u3002</p> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Mount Using Kernel Driver </li> <li>Ceph - Do not mount on the same node </li> </ul>"},{"location":"operations.html#\u914d\u7f6e-cephfs","title":"\u914d\u7f6e cephfs","text":"<p>cephfs \u652f\u6301\u7684\u6700\u5927\u6587\u4ef6\u5927\u5c0f\u9ed8\u8ba4\u662f 1TiB\uff0c\u53ef\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u4fee\u6539\uff1a</p> <pre><code>sudo ceph fs set &lt;fs-name&gt; max_file_size &lt;num-bytes&gt;\n</code></pre> <p>\u4f8b\u5982\uff0c\u4fee\u6539\u4e3a 4TiB\uff1a</p> <pre><code>sudo ceph fs set &lt;fs-name&gt; max_file_size 4398046511104\n</code></pre>"},{"location":"operations.html#\u67e5\u770b-osd","title":"\u67e5\u770b osd","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u96c6\u7fa4\u4e2d\u6240\u6709\u7684 osd \u548c pool\uff1a</p> <pre><code>sudo ceph osd dump\n</code></pre>"},{"location":"operations.html#\u67e5\u770b-pg","title":"\u67e5\u770b pg","text":"<p>\u67e5\u770b\u6240\u6709 pg \u60c5\u51b5\uff1a</p> <pre><code>sudo ceph pg dump\n</code></pre> <p>\u67e5\u770b\u67d0\u4e2a pool \u4e2d\u7684\u6240\u6709 pg\uff1a</p> <pre><code>sudo ceph pg ls-by-pool &lt;pool-name&gt;\n</code></pre> <p>\u67e5\u770b\u67d0\u4e2a osd \u4e2d\u7684\u6240\u6709 pg</p> <pre><code>sudo ceph pg ls-by-osd &lt;pool-name&gt;\n</code></pre> <p>\u67e5\u770b\u5b58\u5728\u5f02\u5e38\u7684\u6240\u6709 pg\uff1a</p> <pre><code>sudo ceph pg dump_stuck unclean\n</code></pre> <p>\u67e5\u770b\u67d0\u4e2a pg \u7684\u8be6\u7ec6\u60c5\u51b5\uff1a</p> <pre><code>sudo ceph pg &lt;pg-id&gt; query\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Placement Groups</li> <li>Ceph - Monitoring OSDs and PGs</li> </ul>"},{"location":"operations.html#\u8c03\u6574-pg-\u6570\u91cf","title":"\u8c03\u6574 pg \u6570\u91cf","text":"<p>\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u5efa\u8bae\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u7981\u7528 pg autoscaling\uff0c\u4ee5\u4fbf\u96c6\u7fa4\u6b63\u5e38\u8fd0\u884c\uff1a</p> <pre><code>sudo ceph config set osd osd_pool_default_pg_autoscale_mode off\n</code></pre> <p>\u7981\u7528 pg autoscaling \u540e\uff0c\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u8bbe\u7f6e\u6bcf\u4e2a pool \u7684 pg \u6570\u91cf\uff1a</p> <pre><code>sudo ceph osd pool set &lt;pool-name&gt; pg_num &lt;num&gt;\nsudo ceph osd pool set &lt;pool-name&gt; pgp_num &lt;num&gt;\n</code></pre> <p>\u4f8b\u5982\uff1a</p> <pre><code>sudo ceph osd pool set default.rgw.buckets.data pg_num 256\nsudo ceph osd pool set default.rgw.buckets.data pgp_num 256\n</code></pre> <p>\u6839\u636e Red Hat \u6587\u6863\uff0c\u63a8\u8350\u6bcf\u4e2a osd \u7ea6 100~200 \u4e2a pg\u3002</p>"},{"location":"operations.html#\u4fee\u590d-pg","title":"\u4fee\u590d pg","text":"<p>\u5f53\u96c6\u7fa4\u7684\u5b58\u50a8\u7b56\u7565\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u6570\u636e\u5728\u91cd\u65b0\u5e73\u8861\u7684\u8fc7\u7a0b\u4e2d\uff0cpg \u53ef\u80fd\u5361\u4f4f\uff0c\u53ef\u5c1d\u8bd5\u4ee5\u4e0b\u65b9\u5f0f\u4fee\u590d pg\uff1a</p> <pre><code>sudo ceph pg force-backfill &lt;pg-id&gt;\n\nsudo ceph pg force-recovery &lt;pg-id&gt;\n\nsudo ceph pg repeer &lt;pg-id&gt;\n</code></pre>"},{"location":"operations.html#\u624b\u52a8-scrub--deep-scrub","title":"\u624b\u52a8 scrub / deep scrub","text":"<p>scrub \u662f\u6307 Ceph \u5bf9 pg \u4e2d\u7684\u6570\u636e\u8fdb\u884c\u4e00\u81f4\u6027\u9a8c\u8bc1\uff0c\u901a\u5e38\u6bcf\u5929\u8fdb\u884c\u4e00\u6b21\u3002</p> <p>deep scrub \u662f\u4e00\u79cd\u66f4\u6df1\u5c42\u6b21\u7684 scrub\uff0c\u4f1a\u8bfb\u53d6\u6570\u636e\u7684\u6bcf\u4e2a bit \u5e76\u8ba1\u7b97 checksum\uff0c\u901a\u5e38\u6bcf\u5468\u8fdb\u884c\u4e00\u6b21\u3002</p> <p>\u5bf9\u67d0\u4e2a pg \u8fdb\u884c scrub / deep scrub\uff1a</p> <pre><code>sudo ceph pg scrub &lt;pg-id&gt;\nsudo ceph pg deep-scrub &lt;pg-id&gt;\n</code></pre> <p>\u5bf9\u67d0\u4e2a osd \u4e0a\u7684\u6240\u6709 pg \u8fdb\u884c scrub / deep scrub\uff1a</p> <pre><code>sudo ceph osd scrub &lt;osd-id&gt;\nsudo ceph osd deep-scrub &lt;osd-id&gt;\n</code></pre> <p>\u5bf9\u67d0\u4e2a pool \u4e2d\u7684\u6240\u6709 pg \u8fdb\u884c scrub / deep scrub\uff1a</p> <pre><code>sudo ceph osd pool scrub &lt;pool-name&gt;\nsudo ceph osd pool deep-scrub &lt;pool-name&gt;\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Scrub a PG</li> </ul>"},{"location":"operations.html#\u4fee\u6539\u96c6\u7fa4\u914d\u7f6e","title":"\u4fee\u6539\u96c6\u7fa4\u914d\u7f6e","text":"<p>Ceph \u96c6\u7fa4\u6709\u975e\u5e38\u591a\u7684\u914d\u7f6e\u53ef\u4ee5\u5b9e\u65f6\u4fee\u6539\u3002</p> <p>\u67e5\u770b\u67d0\u9879\u914d\u7f6e\u7684\u547d\u4ee4\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>sudo ceph config get &lt;who&gt; &lt;config-key&gt;\n</code></pre> <p>\u4fee\u6539\u67d0\u9879\u914d\u7f6e\u7684\u547d\u4ee4\u683c\u5f0f\u5982\u4e0b\uff1a</p> <pre><code>sudo ceph config set &lt;who&gt; &lt;config-key&gt; &lt;config-value&gt;\n</code></pre> <p>\u5176\u4e2d\uff0c<code>&lt;who&gt;</code> \u8868\u793a\u8be5\u9879\u914d\u7f6e\u5bf9\u5e94\u7684\u670d\u52a1\uff0c\u5982 mon\u3001osd \u7b49\u3002</p> <p>\u4f8b\u5982\uff0c\u83b7\u53d6 osd \u4e2d\u6700\u591a\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u591a\u5c11\u4e2a backfill \u64cd\u4f5c\uff1a</p> <pre><code>sudo ceph config get osd osd_max_backfills\n</code></pre> <p>\u8bbe\u7f6e\u8be5\u9879\u914d\u7f6e\u7684\u503c\u4e3a 8\uff1a</p> <pre><code>sudo ceph config set osd osd_max_backfills 8\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Configuring Ceph</li> </ul>"},{"location":"operations.html#\u8c03\u6574\u6570\u636e\u5e73\u8861\u901f\u5ea6","title":"\u8c03\u6574\u6570\u636e\u5e73\u8861\u901f\u5ea6","text":"<p>\u5f53\u96c6\u7fa4\u7684\u5b58\u50a8\u7b56\u7565\u53d1\u751f\u53d8\u5316\u65f6\uff0c\u6570\u636e\u4f1a\u5728 osd \u4e4b\u95f4\u8fdb\u884c\u91cd\u65b0\u5e73\u8861\u3002\u5982\u679c\u671f\u671b\u5c3d\u5feb\u5b8c\u6210\u6570\u636e\u5e73\u8861\uff0c\u53ef\u4ee5\u8c03\u6574\u4ee5\u4e0b\u8bbe\u7f6e\uff1a</p> Name     Default     Now     osd_max_backfills     1     8     osd_recovery_max_active_hdd     3     8     osd_recovery_priority     5     1     osd_recovery_op_priority     3     1     osd_recovery_max_single_start     1     8     osd_recovery_sleep_hdd     0.1     0     <p>\u6ce8\u610f\uff1a\u8c03\u6574\u4e0a\u8ff0\u914d\u7f6e\u4f1a\u6781\u5927\u5730\u5f71\u54cd\u6b63\u5e38\u7684\u8bfb\u5199\u901f\u5ea6\uff0c\u6700\u597d\u5728\u65e0\u4eba\u4f7f\u7528\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\uff0c\u5e76\u5728\u6570\u636e\u5e73\u8861\u5b8c\u6210\u540e\u91cd\u65b0\u8c03\u6574\u4e3a\u9ed8\u8ba4\u503c\u3002</p>"},{"location":"operations.html#\u5220\u9664\u96c6\u7fa4","title":"\u5220\u9664\u96c6\u7fa4","text":"<p>\u6ce8\u610f\uff0c\u6b64\u4e3a\u5371\u9669\u64cd\u4f5c\uff0c\u6570\u636e\u5c06\u5168\u90e8\u4e22\u5931\u3002</p> <p>\u5982\u679c\u786e\u8ba4\u9700\u8981\u5220\u9664\u6574\u4e2a Ceph \u96c6\u7fa4\uff0c\u53ef\u4ee5\u4ece\u975e\u7ba1\u7406\u8282\u70b9\u5f00\u59cb\uff0c\u9010\u4e00\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</p> <pre><code>sudo apt update\nsudo apt install ceph-deploy -y\nsudo ceph-deploy purge &lt;hostname&gt;\nsudo rm -rf /var/lib/ceph/*\n</code></pre> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u683c\u5f0f\u5316 Ceph \u4f7f\u7528\u8fc7\u7684\u5b58\u50a8\u8bbe\u5907\uff1a</p> <pre><code>sudo sgdisk --zap-all &lt;device-path&gt;\n</code></pre> <p>\u4f8b\u5982\uff1a</p> <pre><code>sudo sgdisk --zap-all /dev/sdb\n</code></pre> <p>\u7136\u540e\u91cd\u542f\u8282\u70b9\uff0c\u624d\u80fd\u6062\u590d\u5b58\u50a8\u8bbe\u5907\u7684\u72b6\u6001\uff1a</p> <pre><code>sudo reboot\n</code></pre>"},{"location":"s3.html","title":"S3 \u96c6\u6210","text":""},{"location":"s3.html#\u521b\u5efa-s3-\u670d\u52a1","title":"\u521b\u5efa S3 \u670d\u52a1","text":"<p>\u5728\u7ba1\u7406\u8282\u70b9\u4e0a\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a cephs3 \u7684 RGW \u670d\u52a1\uff08RadosGateWay\uff0c\u7528\u4e8e\u5bf9\u5916\u63d0\u4f9b\u517c\u5bb9 S3 \u7684\u5bf9\u8c61\u5b58\u50a8\u670d\u52a1\uff09\uff1a</p> <pre><code>sudo ceph orch apply rgw cephs3\n</code></pre> <p>\u7136\u540e\u521b\u5efa\u4e00\u4e2a\u80fd\u591f\u8bbf\u95ee\u8be5 RGW \u670d\u52a1\u7684\u7528\u6237\uff0c\u540d\u4e3a demo-s3-user\uff1a</p> <pre><code>$ sudo radosgw-admin user create --uid=demo-s3-user --display-name=demo-s3-user --system\n{\n    \"user_id\": \"demo-s3-user\",\n    \"display_name\": \"demo-s3-user\",\n    \"email\": \"\",\n    \"suspended\": 0,\n    \"max_buckets\": 1000,\n    \"subusers\": [],\n    \"keys\": [\n        {\n            \"user\": \"t9k\",\n            \"access_key\": \"ZVXDTZLORTYJHM8KMY4A\",\n            \"secret_key\": \"T94j1EqxXZ3pc0jPbm3kNREcLfZADMMYDiim4UOq\"\n        }\n    ],\n    \"swift_keys\": [],\n    \"caps\": [],\n    \"op_mask\": \"read, write, delete\",\n    \"system\": \"true\",\n    \"default_placement\": \"\",\n    \"default_storage_class\": \"\",\n    \"placement_tags\": [],\n    \"bucket_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"user_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"temp_url_keys\": [],\n    \"type\": \"rgw\",\n    \"mfa_ids\": []\n}\n</code></pre> <p>\u6ce8\u610f\uff0c\u4e0a\u9762\u751f\u6210\u7684 access_key \u548c secret_key \u5373\u53ef\u7528\u4e8e\u901a\u8fc7 S3 \u534f\u8bae\u8bbf\u95ee\u5bf9\u8c61\u5b58\u50a8\u670d\u52a1\u3002</p> <p>\u914d\u7f6e Web UI \u80fd\u591f\u5c55\u793a\u5bf9\u8c61\u5b58\u50a8\u670d\u52a1\u7684\u8be6\u7ec6\u4fe1\u606f\uff1a</p> <pre><code>sudo ceph dashboard set-rgw-api-access-key -i &lt;file-containing-access-key&gt;\nsudo ceph dashboard set-rgw-api-secret-key -i &lt;file-containing-secret-key&gt;\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - RGW Service</li> </ul>"},{"location":"s3.html#\u914d\u7f6e-erasure-code","title":"\u914d\u7f6e erasure code","text":"<p>\u5982\u679c\u9700\u8981\u8282\u7701\u5b58\u50a8\u7a7a\u95f4\uff0c\u53ef\u4ee5\u914d\u7f6e rgw \u4f7f\u7528 erasure code pool \u6765\u5b58\u50a8\u6570\u636e\u3002</p> <p>\u6ce8\u610f\uff0c\u9700\u8981\u5728 rgw \u5c1a\u672a\u88ab\u4f7f\u7528\u65f6\u5b8c\u6210\u6b64\u64cd\u4f5c\u3002</p> <p>\u9996\u5148\uff0c\u5217\u4e3e rgw \u6240\u4f7f\u7528\u7684 pool\uff0crgw \u901a\u5e38\u4f1a\u521b\u5efa\u540d\u79f0\u4ee5 default.rgw \u5f00\u5934\u7684\u591a\u4e2a pool\uff1a</p> <pre><code>$ sudo ceph osd lspools\n... \n8 default.rgw.log\n9 default.rgw.control\n10 default.rgw.meta\n11 default.rgw.buckets.index\n12 default.rgw.buckets.data\n... \n</code></pre> <p>\u5176\u4e2d\uff0c<code>default.rgw.buckets.data</code> \u662f\u4e3b\u8981\u7528\u6765\u5b58\u50a8\u6570\u636e\u7684 pool\uff0c\u5b83\u9ed8\u8ba4\u662f replicate \u7c7b\u578b\u3002\u7531\u4e8e replicate pool \u4e0d\u80fd\u76f4\u63a5\u8f6c\u6362\u4e3a erasure code pool\uff0c\u6211\u4eec\u9700\u8981\u505c\u6b62 rgw daemon\uff0c\u5220\u9664 data pool\uff0c\u91cd\u65b0\u521b\u5efa data pool \u4e3a erasure code \u7c7b\u578b\uff0c\u6700\u540e\u91cd\u65b0\u521b\u5efa rgw daemon\u3002</p> <pre><code>sudo ceph orch rm rgw.cephs3.us-east-1\nsudo ceph config set mon mon_allow_pool_delete true\nsudo ceph osd pool rm default.rgw.buckets.data default.rgw.buckets.data --yes-i-really-really-mean-it\nsudo ceph osd pool create default.rgw.buckets.data erasure ecprofile-k2-m1-hdd\nsudo ceph osd pool application enable default.rgw.buckets.data rgw\nsudo ceph orch apply rgw cephs3\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>\u914d\u7f6e erasure code</li> <li>Ceph - Erasure code</li> </ul>"},{"location":"s3.html#\u914d\u7f6e-s3-\u670d\u52a1","title":"\u914d\u7f6e S3 \u670d\u52a1","text":"<p>\u5982\u679c\u9700\u8981\u6307\u5b9a rgw \u670d\u52a1\u7684\u8be6\u7ec6\u914d\u7f6e\uff0c\u53ef\u4ee5\u901a\u8fc7\u5e94\u7528 yaml \u914d\u7f6e\u6587\u4ef6\u6765\u5b9e\u65f6\u4fee\u6539 rgw \u670d\u52a1\u3002</p> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u51fa rgw \u670d\u52a1\u7684\u8be6\u7ec6\u914d\u7f6e\uff1a</p> <pre><code>sudo ceph orch ls --service_type rgw --export &gt; rgw.yaml\n</code></pre> <p>rgw.yaml \u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>service_type: rgw\nservice_id: cephs3\nservice_name: rgw.cephs3\nplacement:\n  hosts:\n  - host1\n</code></pre> <p>\u4fee\u6539 rgw.yaml \u4e3a\u4ee5\u4e0b\u5185\u5bb9\uff1a</p> <pre><code>service_type: rgw\nservice_id: cephs3\nservice_name: rgw.cephs3\nplacement:\n  count: 2\n  hosts:\n  - host1\n  - host2\nnetworks:\n- 172.0.0.0/24\nspec:\n  rgw_frontend_port: 8081\n</code></pre> <p>\u5176\u4e2d\uff1a * \u914d\u7f6e rgw daemon \u4e3a 2 \u4e2a\uff0chost1 \u8282\u70b9\u3001host2 \u8282\u70b9\u5404\u4e00\u4e2a * \u914d\u7f6e rgw daemon \u4f7f\u7528 172.0.0.0/24 \u7f51\u7edc * \u914d\u7f6e rgw daemon \u4f7f\u7528 8081 \u7aef\u53e3</p> <p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5e94\u7528\u8be5\u914d\u7f6e\u6587\u4ef6\uff1a</p> <pre><code>sudo ceph orch apply -i ./rgw.yaml\n</code></pre>"},{"location":"s3.html#\u4f7f\u7528-s3-\u670d\u52a1","title":"\u4f7f\u7528 S3 \u670d\u52a1","text":"<p>\u9996\u5148\u60a8\u9700\u8981\u83b7\u53d6 rgw \u670d\u52a1\u7684\u5730\u5740\u3002</p> <p>\u53c2\u8003\u67e5\u770b daemon \u72b6\u6001\uff0c\u67e5\u770b rgw \u670d\u52a1\u7684\u914d\u7f6e\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>$ ceph orch ls\nNAME            PORTS        RUNNING  REFRESHED  AGE  PLACEMENT\nrgw.cephs3      ?:8081           2/2  6m ago     2w   host1;host2;count:2\n</code></pre> <p>\u53ef\u4ee5\u770b\u5230 rgw \u670d\u52a1\u8fd0\u884c\u5728 host1 \u548c host2 \u8282\u70b9\u7684 8081 \u7aef\u53e3\u3002</p> <p>\u53c2\u8003\u67e5\u770b\u8282\u70b9\uff0c\u67e5\u770b\u8282\u70b9\u7684 ip \u5730\u5740\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>$ ceph orch host ls\nHOST   ADDR         LABELS  STATUS\nhost1  172.0.0.1\nhost2  172.0.0.2\n</code></pre> <p>\u56e0\u6b64\uff0crgw \u670d\u52a1\u7684\u5730\u5740\u4e3a <code>http://172.0.0.1:8081</code> \u548c <code>http://172.0.0.1:8081</code>\uff0c\u4efb\u9009\u5176\u4e2d\u4e00\u4e2a\u8bbf\u95ee\u5373\u53ef\u3002</p> <p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u901a\u8fc7 S3 \u534f\u8bae\u8bbf\u95ee rgw \u670d\u52a1\uff1a</p> <pre><code>$ cat &gt; ~/.s3cfg &lt;&lt; EOF\nhost_base = http://172.0.0.1:8081\nhost_bucket = http://172.0.0.1:8081\nbucket_location = us-east-1\nuse_https = False\naccess_key = ZVXDTZLOFRTJHM8KMY4A\nsecret_key = T94j1EqxXZ3pl9jkjm3kNREcLfZADMMYDiim4UOq\nsignature_v2 = False\nEOF\n$ s3cmd ls\n$ s3cmd mb s3://mybucket\n$ s3cmd ls\n</code></pre>"},{"location":"s3.html#\u914d\u989d\u7ba1\u7406","title":"\u914d\u989d\u7ba1\u7406","text":"<p>\u5bf9\u4e8e\u5bf9\u8c61\u5b58\u50a8\u670d\u52a1\uff0cCeph \u652f\u6301\u9488\u5bf9 bucket \u6216\u8005 user \u8fdb\u884c\u914d\u989d\u7ba1\u7406\u3002</p> <p>\u5728\u4efb\u610f\u7ba1\u7406\u8282\u70b9\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5bf9\u540d\u4e3a demo-s3-user \u7684 user \u8bbe\u7f6e\u914d\u989d\uff1a</p> <pre><code>sudo radosgw-admin quota set --quota-scope=user --uid=demo-s3-user --max-size=1024B\nsudo radosgw-admin quota enable --quota-scope=user --uid=demo-s3-user\n</code></pre> <p>\u5bf9\u540d\u4e3a demo-s3-user \u7684 user \u6240\u521b\u5efa\u7684\u6240\u6709 bucket \u8bbe\u7f6e\u914d\u989d\uff1a</p> <pre><code>sudo radosgw-admin quota set --quota-scope=bucket --uid=demo-s3-user --max-size=1024B\nsudo radosgw-admin quota enable --quota-scope=bucket --uid=demo-s3-user\n</code></pre> <p>\u67e5\u770b demo-s3-user \u7684\u914d\u989d\u8bbe\u7f6e\uff1a</p> <pre><code>sudo radosgw-admin user info --uid=demo-s3-user\n</code></pre> <p>\u53c2\u8003\u6587\u6863\uff1a</p> <ul> <li>Ceph - Quota Management</li> </ul>"},{"location":"troubleshooting.html","title":"\u5e38\u89c1\u6545\u969c","text":""},{"location":"troubleshooting.html#pvc-\u521b\u5efa\u7ed1\u5b9a\u5931\u8d25","title":"PVC \u521b\u5efa/\u7ed1\u5b9a\u5931\u8d25","text":"<p>\u53c2\u8003\u67e5\u770b PVC \u521b\u5efa/\u7ed1\u5b9a\u5931\u8d25\u539f\u56e0\u3002</p>"},{"location":"troubleshooting.html#k8s-\u8282\u70b9\u7f51\u7edc\u65ad\u5f00\u540e\u6062\u590d\u4f46\u662f\u8fd0\u884c\u5728\u8282\u70b9\u4e0a\u7684-pod-\u4ecd\u7136\u62a5\u9519","title":"K8s \u8282\u70b9\u7f51\u7edc\u65ad\u5f00\u540e\u6062\u590d\uff0c\u4f46\u662f\u8fd0\u884c\u5728\u8282\u70b9\u4e0a\u7684 Pod \u4ecd\u7136\u62a5\u9519","text":"<p>\u5982\u679c K8s \u96c6\u7fa4\u7f51\u7edc\u65ad\u5f00\u540e\u6062\u590d\uff0c\u4f46\u662f Pod \u51fa\u73b0\u5982\u4e0b\u62a5\u9519\uff1a</p> <pre><code>$ k describe po -n aim-yyx managed-notebook-c26a6-0\nEvents:\n  Type     Reason       Age                    From           Message\n  ----     ------       ----                   ----           -------\n  Normal   Scheduled    11m                    t9k-scheduler  Successfully assigned demo/managed-notebook-c26a6-0 to node1\n  Warning  FailedMount  7m38s (x2 over 9m54s)  kubelet        Unable to attach or mount volumes: unmounted volumes=[datadir], unattached volumes=[pep-config workingdir datadir managed-notebook-c26a6 kube-api-access-2wbpn]: timed out waiting for the condition\n  Warning  FailedMount  3m2s (x2 over 5m20s)   kubelet        Unable to attach or mount volumes: unmounted volumes=[datadir], unattached volumes=[pep-config workingdir datadir managed-notebook-c26a6 kube-api-access-2wbpn]: timed out waiting for the condition\n  Warning  FailedMount  99s (x13 over 11m)     kubelet        MountVolume.SetUp failed for volume \"pvc-003a5630-b230-406f-9493-2f66653f768c\" : rpc error: code = Internal desc = stat /var/lib/kubelet/plugins/kubernetes.io/csi/cephfs.csi.ceph.com/d542aa1dad943aea439969def75da7d1ded28a5fcdbae0657f2b7bd0dd98c694/globalmount: permission denied\n  Warning  FailedMount  48s                    kubelet        Unable to attach or mount volumes: unmounted volumes=[datadir], unattached volumes=[pep-config workingdir datadir managed-notebook-c26a6 kube-api-access-2wbpn]: timed out waiting for the condition\n</code></pre> <p>\u6839\u636e GitHub issue\uff0c\u539f\u56e0\u662f\u56e0\u4e3a ceph-csi \u6ca1\u6709\u6b63\u786e\u5904\u7406 mount point \u7531\u4e8e\u957f\u65f6\u95f4\u6ca1\u6709\u54cd\u5e94\u88ab linux kernel \u9a71\u9010\u7684\u60c5\u51b5\u3002</p> <p>\u6682\u65f6\u7684\u89e3\u51b3\u529e\u6cd5\u662f\u91cd\u542f Pod \u6240\u5728\u7684\u8282\u70b9\uff1a</p> <pre><code>kubectl drain &lt;node&gt;\n# reboot &lt;node&gt;\nkubectl uncordon &lt;node&gt;\n</code></pre>"},{"location":"troubleshooting.html#mpath-\u8bbe\u5907\u6240\u5728\u8282\u70b9\u91cd\u542f\u540eosd-\u62a5\u9519","title":"mpath \u8bbe\u5907\u6240\u5728\u8282\u70b9\u91cd\u542f\u540e\uff0cOSD \u62a5\u9519","text":"<p>\u53c2\u8003\u96c6\u7fa4\u5b89\u88c5 - mpath \u8bbe\u5907\uff0c\u5f53 mpath \u8bbe\u5907\u6240\u5728\u7684\u8282\u70b9\u91cd\u542f\u65f6\uff0clv \u53ef\u80fd\u53d8\u4e3a unavailable \u72b6\u6001\uff0c\u5bfc\u81f4 osd \u5f02\u5e38\uff0c\u9700\u8981\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u91cd\u65b0\u6fc0\u6d3b lv\uff1a</p> <pre><code>sudo lvchange -ay /dev/vgmpatha/lv0\n</code></pre>"}]}